{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Find duplicate questions on StackOverflow by their embeddings\n",
    "\n",
    "In this assignment you will learn how to calculate a similarity for pieces of text. Using this approach you will know how to find duplicate questions from [StackOverflow](https://stackoverflow.com)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Libraries\n",
    "\n",
    "In this task you will you will need the following libraries:\n",
    "- [StarSpace](https://github.com/facebookresearch/StarSpace) — a general-purpose model for efficient learning of entity embeddings from Facebook\n",
    "- [Gensim](https://radimrehurek.com/gensim/) — a tool for solving various NLP-related tasks (topic modeling, text representation, ...)\n",
    "- [Numpy](http://www.numpy.org) — a package for scientific computing.\n",
    "- [scikit-learn](http://scikit-learn.org/stable/index.html) — a tool for data mining and data analysis.\n",
    "- [Nltk](http://www.nltk.org) — a platform to work with human language data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data\n",
    "\n",
    "The following cell will download all data required for this assignment into the folder `week3/data`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File data/train.tsv is already downloaded.\n",
      "File data/validation.tsv is already downloaded.\n",
      "File data/test.tsv is already downloaded.\n",
      "File data/test_embeddings.tsv is already downloaded.\n",
      "Downloading GoogleNews-vectors-negative300.bin.gz (1.5G) for you, it will take a while...\n",
      "**************************************************\n",
      "GoogleNews-vectors-negative300.bin.gz\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "sys.path.append(\"..\")\n",
    "from common.download_utils import download_week3_resources\n",
    "\n",
    "download_week3_resources()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Word embedding\n",
    "\n",
    "To solve the problem, you will use two different models of embeddings:\n",
    "\n",
    " - [Pre-trained word vectors](https://code.google.com/archive/p/word2vec/) from Google which were trained on a part of Google News dataset (about 100 billion words). The model contains 300-dimensional vectors for 3 million words and phrases. `GoogleNews-vectors-negative300.bin.gz` will be downloaded in `download_week3_resources()`.\n",
    " - Representations using StarSpace on StackOverflow data sample. You will need to train them from scratch."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It's always easier to start with pre-trained embeddings. Unpack the pre-trained Goggle's vectors and upload them using the function [KeyedVectors.load_word2vec_format](https://radimrehurek.com/gensim/models/keyedvectors.html) from gensim library with the parameter *binary=True*. If the size of the embeddings is larger than the avaliable memory, you could load only a part of the embeddings by defining the parameter *limit* (recommended: 500000)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data\t\t\t\t       grader.py  week3-Embeddings.ipynb\r\n",
      "GoogleNews-vectors-negative300.bin.gz  util.py\r\n"
     ]
    }
   ],
   "source": [
    "!ls ./"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "wv_embeddings = gensim.models.KeyedVectors.load_word2vec_format('GoogleNews-vectors-negative300.bin.gz', binary = True, limit = 500000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### How to work with Google's word2vec embeddings?\n",
    "\n",
    "Once you have loaded the representations, make sure you can access them. First, you can check if the loaded embeddings contain a word:\n",
    "    \n",
    "    'word' in wv_embeddings\n",
    "    \n",
    "Second, to get the corresponding embedding you can use the square brackets:\n",
    "\n",
    "    wv_embeddings['word']\n",
    " \n",
    "### Checking that the embeddings are correct \n",
    " \n",
    "To prevent any errors during the first stage, we can check that the loaded embeddings are correct. You can call the function *check_embeddings*, implemented below, which runs 3 tests:\n",
    "1. Find the most similar word for provided \"positive\" and \"negative\" words.\n",
    "2. Find which word from the given list doesn’t go with the others.\n",
    "3. Find the most similar word for the provided one.\n",
    "\n",
    "In the right case the function will return the string *These embeddings look good*. Othervise, you need to validate the previous steps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_embeddings(embeddings):\n",
    "    error_text = \"Something wrong with your embeddings ('%s test isn't correct).\"\n",
    "    most_similar = embeddings.most_similar(positive=['woman', 'king'], negative=['man'])\n",
    "    if len(most_similar) < 1 or most_similar[0][0] != 'queen':\n",
    "        return error_text % \"Most similar\"\n",
    "\n",
    "    doesnt_match = embeddings.doesnt_match(['breakfast', 'cereal', 'dinner', 'lunch'])\n",
    "    if doesnt_match != 'cereal':\n",
    "        return error_text % \"Doesn't match\"\n",
    "    \n",
    "    most_similar_to_given = embeddings.most_similar_to_given('music', ['water', 'sound', 'backpack', 'mouse'])\n",
    "    if most_similar_to_given != 'sound':\n",
    "        return error_text % \"Most similar to given\"\n",
    "    \n",
    "    return \"These embeddings look good.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "These embeddings look good.\n"
     ]
    }
   ],
   "source": [
    "print(check_embeddings(wv_embeddings))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## From word to text embeddings\n",
    "\n",
    "**Task 1 (Question2Vec).** Usually, we have word-based embeddings, but for the task we need to create a representation for the whole question. It could be done in different ways. In our case we will use a **mean** of all word vectors in the question. Now you need to implement the function *question_to_vec*, which calculates the question representation described above. This function should work with the input text as is without any preprocessing.\n",
    "\n",
    "Note that there could be words without the corresponding embeddings. In this case, you can just skip these words and don't take them into account during calculating the result. If the question doesn't contain any known word with embedding, the function should return a zero vector."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "def question_to_vec(question, embeddings, dim=300):\n",
    "    \"\"\"\n",
    "        question: a string\n",
    "        embeddings: dict where the key is a word and a value is its' embedding\n",
    "        dim: size of the representation\n",
    "\n",
    "        result: vector representation for the question\n",
    "    \"\"\"\n",
    "    ######################################\n",
    "    ######### YOUR CODE HERE #############\n",
    "    ######################################\n",
    "    \n",
    "    out = np.zeros(dim)\n",
    "    num_words = 0\n",
    "    words = question.split(' ')\n",
    "    for word in words:\n",
    "        if word not in embeddings:\n",
    "            continue\n",
    "        out += embeddings[word]\n",
    "        num_words += 1\n",
    "    if num_words == 0:\n",
    "        num_words = 1\n",
    "    return out/num_words\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To check the basic correctness of your implementation, run the function *question_to_vec_tests*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "def question_to_vec_tests():\n",
    "    if (np.zeros(300) != question_to_vec('', wv_embeddings)).any():\n",
    "        return \"You need to return zero vector for empty question.\"\n",
    "    if (np.zeros(300) != question_to_vec('thereisnosuchword', wv_embeddings)).any():\n",
    "        return \"You need to return zero vector for the question, which consists only unknown words.\"\n",
    "    if (wv_embeddings['word'] != question_to_vec('word', wv_embeddings)).any():\n",
    "        return \"You need to check the corectness of your function.\"\n",
    "    if ((wv_embeddings['I'] + wv_embeddings['am']) / 2 != question_to_vec('I am', wv_embeddings)).any():\n",
    "        return \"Your function should calculate a mean of word vectors.\"\n",
    "    if (wv_embeddings['word'] != question_to_vec('thereisnosuchword word', wv_embeddings)).any():\n",
    "        return \"You should not consider words which embeddings are unknown.\"\n",
    "    return \"Basic tests are passed.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Basic tests are passed.\n"
     ]
    }
   ],
   "source": [
    "print(question_to_vec_tests())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can submit embeddings for the questions from the file *test_embeddings.tsv* to earn the points. In this task you don't need to transform the text of a question somehow."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /home/ubuntu/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "from util import array_to_string"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we have a method to create a representation of any sentence and we are ready for the first evaluation. So, let's check how well our solution (Google's vectors + *question_to_vec*) will work.\n",
    "\n",
    "## Evaluation of text similarity\n",
    "\n",
    "We can imagine that if we use good embeddings, the cosine similarity between the duplicate sentences should be less than for the random ones. Overall, for each pair of duplicate sentences we can generate *R* random negative examples and find out the position of the correct duplicate.  \n",
    "\n",
    "For example, we have the question *\"Exceptions What really happens\"* and we are sure that another question *\"How does the catch keyword determine the type of exception that was thrown\"* is a duplicate. But our model doesn't know it and tries to find out the best option also among questions like *\"How Can I Make These Links Rotate in PHP\"*, *\"NSLog array description not memory address\"* and *\"PECL_HTTP not recognised php ubuntu\"*. The goal of the model is to rank all these 4 questions (1 *positive* and *R* = 3 *negative*) in the way that the correct one is in the first place.\n",
    "\n",
    "However, it is unnatural to count on that the best candidate will be always in the first place. So let us consider the place of the best candidate in the sorted list of candidates and formulate a metric based on it. We can fix some *K* — a reasonalble number of top-ranked elements and *N* — a number of queries (size of the sample).\n",
    "\n",
    "### Hits@K\n",
    "\n",
    "The first simple metric will be a number of correct hits for some *K*:\n",
    "$$ \\text{Hits@K} = \\frac{1}{N}\\sum_{i=1}^N \\, [dup_i \\in topK(q_i)]$$\n",
    "\n",
    "where $q_i$ is the i-th query, $dup_i$ is its duplicate, $topK(q_i)$ is the top K elements of the ranked sentences provided by our model and the operation $[dup_i \\in topK(q_i)]$ equals 1 if the condition is true and 0 otherwise (more details about this operation could be found [here](https://en.wikipedia.org/wiki/Iverson_bracket)).\n",
    "\n",
    "\n",
    "### DCG@K\n",
    "The second one is a simplified [DCG metric](https://en.wikipedia.org/wiki/Discounted_cumulative_gain):\n",
    "\n",
    "$$ \\text{DCG@K} = \\frac{1}{N} \\sum_{i=1}^N\\frac{1}{\\log_2(1+rank_{dup_i})}\\cdot[rank_{dup_i} \\le K] $$\n",
    "\n",
    "where $rank_{dup_i}$ is a position of the duplicate in the sorted list of the nearest sentences for the query $q_i$. According to this metric, the model gets a higher reward for a higher position of the correct answer. If the answer does not appear in topK at all, the reward is zero. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluation examples\n",
    "\n",
    "Let's calculate the described metrics for the toy example introduced above. In this case $N$ = 1 and the correct candidate for $q_1$ is *\"How does the catch keyword determine the type of exception that was thrown\"*. Consider the following ranking of the candidates:\n",
    "1. *\"How Can I Make These Links Rotate in PHP\"*\n",
    "2. *\"How does the catch keyword determine the type of exception that was thrown\"*\n",
    "3. *\"NSLog array description not memory address\"*\n",
    "4. *\"PECL_HTTP not recognised php ubuntu\"*\n",
    "\n",
    "Using the ranking above, calculate *Hits@K* metric for *K = 1, 2, 4*: \n",
    " \n",
    "- [K = 1] $\\text{Hits@1} = \\frac{1}{1}\\sum_{i=1}^1 \\, [dup_i \\in top1(q_i)] = [dup_1 \\in top1(q_1)] = 0$ because the correct answer doesn't appear in the *top1* list.\n",
    "- [K = 2] $\\text{Hits@2} = \\frac{1}{1}\\sum_{i=1}^1 \\, [dup_i \\in top2(q_i)] = [dup_1 \\in top2(q_1)] = 1$ because $rank_{dup_1} = 2$.\n",
    "- [K = 4] $\\text{Hits@4} = \\frac{1}{1}\\sum_{i=1}^1 \\, [dup_i \\in top4(q_i)] = [dup_1 \\in top4(q_1)] = 1$\n",
    "\n",
    "Using the ranking above, calculate *DCG@K* metric for *K = 1, 2, 4*:\n",
    "\n",
    "- [K = 1] $\\text{DCG@1} = \\frac{1}{1} \\sum_{i=1}^1\\frac{1}{\\log_2(1+rank_{dup_i})}\\cdot[rank_{dup_i} \\le 1] = \\frac{1}{\\log_2(1+rank_{dup_i})}\\cdot[rank_{dup_i} \\le 1] = 0$ because the correct answer doesn't appear in the top1 list.\n",
    "- [K = 2] $\\text{DCG@2} = \\frac{1}{1} \\sum_{i=1}^1\\frac{1}{\\log_2(1+rank_{dup_i})}\\cdot[rank_{dup_i} \\le 2] = \\frac{1}{\\log_2{3}}$, because $rank_{dup_1} = 2$.\n",
    "- [K = 4] $\\text{DCG@4} = \\frac{1}{1} \\sum_{i=1}^1\\frac{1}{\\log_2(1+rank_{dup_i})}\\cdot[rank_{dup_i} \\le 4] = \\frac{1}{\\log_2{3}}$.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Tasks 2 and 3 (HitsCount and DCGScore).** Implement the functions *hits_count* and *dcg_score* as described above. Each function has two arguments: *dup_ranks* and *k*. *dup_ranks* is a list which contains *values of ranks* of duplicates. For example, *dup_ranks* is *[2]* for the example provided above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "def hits_count(dup_ranks, k):\n",
    "    \"\"\"\n",
    "        dup_ranks: list of duplicates' ranks; one rank per question; \n",
    "                   length is a number of questions which we are looking for duplicates; \n",
    "                   rank is a number from 1 to len(candidates of the question); \n",
    "                   e.g. [2, 3] means that the first duplicate has the rank 2, the second one — 3.\n",
    "        k: number of top-ranked elements (k in Hits@k metric)\n",
    "\n",
    "        result: return Hits@k value for current ranking\n",
    "    \"\"\"\n",
    "    ######################################\n",
    "    ######### YOUR CODE HERE #############\n",
    "    ######################################\n",
    "    \n",
    "    hist_ranks = 0\n",
    "    \n",
    "    for dup_rank in dup_ranks:\n",
    "        if dup_rank <= k:\n",
    "            hist_ranks+=1\n",
    "    \n",
    "    return hist_ranks/len(dup_ranks)\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test your code on the tiny examples:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_hits():\n",
    "    # *Evaluation example*\n",
    "    # answers — dup_i\n",
    "    answers = [\"How does the catch keyword determine the type of exception that was thrown\"]\n",
    "    \n",
    "    # candidates_ranking — the ranked sentences provided by our model\n",
    "    candidates_ranking = [[\"How Can I Make These Links Rotate in PHP\", \n",
    "                           \"How does the catch keyword determine the type of exception that was thrown\",\n",
    "                           \"NSLog array description not memory address\",\n",
    "                           \"PECL_HTTP not recognised php ubuntu\"]]\n",
    "    # dup_ranks — position of the dup_i in the list of ranks +1\n",
    "    dup_ranks = [candidates_ranking[i].index(answers[i]) + 1 for i in range(len(answers))]\n",
    "    \n",
    "    # correct_answers — the expected values of the result for each k from 1 to 4\n",
    "    correct_answers = [0, 1, 1, 1]\n",
    "    for k, correct in enumerate(correct_answers, 1):\n",
    "        if not np.isclose(hits_count(dup_ranks, k), correct):\n",
    "            return \"Check the function.\"\n",
    "    \n",
    "    # Other tests\n",
    "    answers = [\"How does the catch keyword determine the type of exception that was thrown\", \n",
    "               \"Convert Google results object (pure js) to Python object\"]\n",
    "    \n",
    "    # The first test: both duplicates on the first position in ranked list\n",
    "    candidates_ranking = [[\"How does the catch keyword determine the type of exception that was thrown\",\n",
    "                           \"How Can I Make These Links Rotate in PHP\"], \n",
    "                          [\"Convert Google results object (pure js) to Python object\",\n",
    "                           \"WPF- How to update the changes in list item of a list\"]]\n",
    "    dup_ranks = [candidates_ranking[i].index(answers[i]) + 1 for i in range(len(answers))]\n",
    "    correct_answers = [1, 1]\n",
    "    for k, correct in enumerate(correct_answers, 1):\n",
    "        if not np.isclose(hits_count(dup_ranks, k), correct):\n",
    "            return \"Check the function (test: both duplicates on the first position in ranked list).\"\n",
    "        \n",
    "    # The second test: one candidate on the first position, another — on the second\n",
    "    candidates_ranking = [[\"How Can I Make These Links Rotate in PHP\", \n",
    "                           \"How does the catch keyword determine the type of exception that was thrown\"], \n",
    "                          [\"Convert Google results object (pure js) to Python object\",\n",
    "                           \"WPF- How to update the changes in list item of a list\"]]\n",
    "    dup_ranks = [candidates_ranking[i].index(answers[i]) + 1 for i in range(len(answers))]\n",
    "    correct_answers = [0.5, 1]\n",
    "    for k, correct in enumerate(correct_answers, 1):\n",
    "        if not np.isclose(hits_count(dup_ranks, k), correct):\n",
    "            return \"Check the function (test: one candidate on the first position, another — on the second).\"\n",
    "\n",
    "    # The third test: both candidates on the second position\n",
    "    candidates_ranking = [[\"How Can I Make These Links Rotate in PHP\", \n",
    "                           \"How does the catch keyword determine the type of exception that was thrown\"], \n",
    "                          [\"WPF- How to update the changes in list item of a list\",\n",
    "                           \"Convert Google results object (pure js) to Python object\"]]\n",
    "    dup_ranks = [candidates_ranking[i].index(answers[i]) + 1 for i in range(len(answers))]\n",
    "    correct_answers = [0, 1]\n",
    "    for k, correct in enumerate(correct_answers, 1):\n",
    "        if not np.isclose(hits_count(dup_ranks, k), correct):\n",
    "            return \"Check the function (test: both candidates on the second position).\"\n",
    "\n",
    "    return \"Basic test are passed.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Basic test are passed.\n"
     ]
    }
   ],
   "source": [
    "print(test_hits())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dcg_score(dup_ranks, k):\n",
    "    \"\"\"\n",
    "        dup_ranks: list of duplicates' ranks; one rank per question; \n",
    "                   length is a number of questions which we are looking for duplicates; \n",
    "                   rank is a number from 1 to len(candidates of the question); \n",
    "                   e.g. [2, 3] means that the first duplicate has the rank 2, the second one — 3.\n",
    "        k: number of top-ranked elements (k in DCG@k metric)\n",
    "\n",
    "        result: return DCG@k value for current ranking\n",
    "    \"\"\"\n",
    "    ######################################\n",
    "    ######### YOUR CODE HERE #############\n",
    "    ######################################\n",
    "    hist_ranks = 0\n",
    "    \n",
    "    for dup_rank in dup_ranks:\n",
    "        if dup_rank <= k:\n",
    "            hist_ranks+= 1/(np.log2(1 + dup_rank))\n",
    "    \n",
    "    return hist_ranks/len(dup_ranks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_dcg():\n",
    "    # *Evaluation example*\n",
    "    # answers — dup_i\n",
    "    answers = [\"How does the catch keyword determine the type of exception that was thrown\"]\n",
    "    \n",
    "    # candidates_ranking — the ranked sentences provided by our model\n",
    "    candidates_ranking = [[\"How Can I Make These Links Rotate in PHP\", \n",
    "                           \"How does the catch keyword determine the type of exception that was thrown\",\n",
    "                           \"NSLog array description not memory address\",\n",
    "                           \"PECL_HTTP not recognised php ubuntu\"]]\n",
    "    # dup_ranks — position of the dup_i in the list of ranks +1\n",
    "    dup_ranks = [candidates_ranking[i].index(answers[i]) + 1 for i in range(len(answers))]\n",
    "    \n",
    "    # correct_answers — the expected values of the result for each k from 1 to 4\n",
    "    correct_answers = [0, 1 / (np.log2(3)), 1 / (np.log2(3)), 1 / (np.log2(3))]\n",
    "    for k, correct in enumerate(correct_answers, 1):\n",
    "        if not np.isclose(dcg_score(dup_ranks, k), correct):\n",
    "            return \"Check the function.\"\n",
    "    \n",
    "    # Other tests\n",
    "    answers = [\"How does the catch keyword determine the type of exception that was thrown\", \n",
    "               \"Convert Google results object (pure js) to Python object\"]\n",
    "\n",
    "    # The first test: both duplicates on the first position in ranked list\n",
    "    candidates_ranking = [[\"How does the catch keyword determine the type of exception that was thrown\",\n",
    "                           \"How Can I Make These Links Rotate in PHP\"], \n",
    "                          [\"Convert Google results object (pure js) to Python object\",\n",
    "                           \"WPF- How to update the changes in list item of a list\"]]\n",
    "    dup_ranks = [candidates_ranking[i].index(answers[i]) + 1 for i in range(len(answers))]\n",
    "    correct_answers = [1, 1]\n",
    "    for k, correct in enumerate(correct_answers, 1):\n",
    "        if not np.isclose(dcg_score(dup_ranks, k), correct):\n",
    "            return \"Check the function (test: both duplicates on the first position in ranked list).\"\n",
    "        \n",
    "    # The second test: one candidate on the first position, another — on the second\n",
    "    candidates_ranking = [[\"How Can I Make These Links Rotate in PHP\", \n",
    "                           \"How does the catch keyword determine the type of exception that was thrown\"], \n",
    "                          [\"Convert Google results object (pure js) to Python object\",\n",
    "                           \"WPF- How to update the changes in list item of a list\"]]\n",
    "    dup_ranks = [candidates_ranking[i].index(answers[i]) + 1 for i in range(len(answers))]\n",
    "    correct_answers = [0.5, (1 + (1 / (np.log2(3)))) / 2]\n",
    "    for k, correct in enumerate(correct_answers, 1):\n",
    "        if not np.isclose(dcg_score(dup_ranks, k), correct):\n",
    "            return \"Check the function (test: one candidate on the first position, another — on the second).\"\n",
    "        \n",
    "    # The third test: both candidates on the second position\n",
    "    candidates_ranking = [[\"How Can I Make These Links Rotate in PHP\",\n",
    "                           \"How does the catch keyword determine the type of exception that was thrown\"], \n",
    "                          [\"WPF- How to update the changes in list item of a list\",\n",
    "                           \"Convert Google results object (pure js) to Python object\"]]\n",
    "    dup_ranks = [candidates_ranking[i].index(answers[i]) + 1 for i in range(len(answers))]\n",
    "    correct_answers = [0, 1 / (np.log2(3))]\n",
    "    for k, correct in enumerate(correct_answers, 1):\n",
    "        if not np.isclose(dcg_score(dup_ranks, k), correct):\n",
    "            return \"Check the function (test: both candidates on the second position).\"\n",
    "\n",
    "    return \"Basic test are passed.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Basic test are passed.\n"
     ]
    }
   ],
   "source": [
    "print(test_dcg())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Submit results of the functions *hits_count* and *dcg_score* for the following examples to earn the points."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  First solution: pre-trained embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will work with predefined train, validation and test corpora. All the files are tab-separated, but have a different format:\n",
    " - *train* corpus contains similar sentences at the same row.\n",
    " - *validation* corpus contains the following columns: *question*, *similar question*, *negative example 1*, *negative example 2*, ... \n",
    " - *test* corpus contains the following columns: *question*, *example 1*, *example 2*, ...\n",
    "\n",
    "Validation corpus will be used for the intermediate validation of models. The test data will be necessary for submitting the quality of your model in the system."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now you should read *validation* corpus, located at `data/validation.tsv`. You will use it later to evaluate current solution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_corpus(filename):\n",
    "    data = []\n",
    "    for line in open(filename, encoding='utf-8'):\n",
    "        data.append(line.strip().split('\\t'))\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test_embeddings.tsv  test.tsv  train.tsv  validation.tsv\r\n"
     ]
    }
   ],
   "source": [
    "!ls ./data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "validation = read_corpus('data/validation.tsv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics.pairwise import cosine_similarity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will use cosine distance to rank candidate questions which you need to implement in the function *rank_candidates*. The function should return a sorted list of pairs *(initial position in candidates list, candidate)*. Index of some pair corresponds to its rank (the first is the best). For example, if the list of candidates was *[a, b, c]* and the most similar is *c*, then *a* and *b*, the function should return a list *[(2, c), (0, a), (1, b)]*.\n",
    "\n",
    "Pay attention, if you use the function *cosine_similarity* from *sklearn.metrics.pairwise* to calculate similarity because it works in a different way: most similar objects has greatest similarity. It's preferable to use a vectorized version of *cosine_similarity* function. Try to compute similarity at once and not use list comprehension. It should speed up your computations significantly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rank_candidates(question, candidates, embeddings, dim=300):\n",
    "    \"\"\"\n",
    "        question: a string\n",
    "        candidates: a list of strings (candidates) which we want to rank\n",
    "        embeddings: some embeddings\n",
    "        dim: dimension of the current embeddings\n",
    "        \n",
    "        result: a list of pairs (initial position in the list, question)\n",
    "    \"\"\"\n",
    "    \n",
    "    ######################################\n",
    "    ######### YOUR CODE HERE #############\n",
    "    ######################################\n",
    "    \n",
    "    q_vector = question_to_vec(question, embeddings, dim)\n",
    "    \n",
    "    q_positions = range(0, len(candidates))\n",
    "    \n",
    "    c_vectors = [question_to_vec(c, embeddings, dim) for c in candidates]\n",
    "    \n",
    "    cos_sim = [cosine_similarity(c_vector.reshape(1, -1), q_vector.reshape(1, -1)) for c_vector in c_vectors]\n",
    "    \n",
    "    combined = list(zip(candidates, cos_sim, q_positions))\n",
    "    \n",
    "    combined = sorted(combined, key = lambda comb: -comb[1])\n",
    "    \n",
    "    combined = [(pos, q) for (q, _, pos) in combined]\n",
    "    \n",
    "    return combined\n",
    "    \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(1, 'C# create cookie from string and send it'),\n",
       " (0, 'Convert Google results object (pure js) to Python object'),\n",
       " (2, 'How to use jQuery AJAX for an outside domain?')]"
      ]
     },
     "execution_count": 158,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "questions = ['converting string to list', 'Sending array via Ajax fails']\n",
    "candidates = [['Convert Google results object (pure js) to Python object', \n",
    "                   'C# create cookie from string and send it',\n",
    "                   'How to use jQuery AJAX for an outside domain?'], \n",
    "                  ['Getting all list items of an unordered list in PHP', \n",
    "                   'WPF- How to update the changes in list item of a list', \n",
    "                   'select2 not displaying search results']]\n",
    "\n",
    "\n",
    "rank_candidates(questions[0], candidates[0], wv_embeddings)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test your code on the tiny examples:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_rank_candidates():\n",
    "    questions = ['converting string to list', 'Sending array via Ajax fails']\n",
    "    candidates = [['Convert Google results object (pure js) to Python object', \n",
    "                   'C# create cookie from string and send it',\n",
    "                   'How to use jQuery AJAX for an outside domain?'], \n",
    "                  ['Getting all list items of an unordered list in PHP', \n",
    "                   'WPF- How to update the changes in list item of a list', \n",
    "                   'select2 not displaying search results']]\n",
    "    results = [[(1, 'C# create cookie from string and send it'), \n",
    "                (0, 'Convert Google results object (pure js) to Python object'), \n",
    "                (2, 'How to use jQuery AJAX for an outside domain?')],\n",
    "               [(0, 'Getting all list items of an unordered list in PHP'), \n",
    "                (2, 'select2 not displaying search results'), \n",
    "                (1, 'WPF- How to update the changes in list item of a list')]]\n",
    "    for question, q_candidates, result in zip(questions, candidates, results):\n",
    "        ranks = rank_candidates(question, q_candidates, wv_embeddings, 300)\n",
    "        if not np.all(ranks == result):\n",
    "            return \"Check the function.\"\n",
    "    return \"Basic tests are passed.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Basic tests are passed.\n"
     ]
    }
   ],
   "source": [
    "print(test_rank_candidates())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can test the quality of the current approach. Run the next two cells to get the results. Pay attention that calculation of similarity between vectors takes time and this calculation is computed approximately in 10 minutes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-110-44ef4450b14b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mline\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mvalidation\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0mq\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0mex\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mline\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m     \u001b[0mranks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrank_candidates\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mq\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mex\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwv_embeddings\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m     \u001b[0mwv_ranking\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mr\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mr\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mranks\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-104-f9a9ad3f3957>\u001b[0m in \u001b[0;36mrank_candidates\u001b[0;34m(question, candidates, embeddings, dim)\u001b[0m\n\u001b[1;32m     19\u001b[0m     \u001b[0mc_vectors\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mquestion_to_vec\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0membeddings\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdim\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mc\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mcandidates\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 21\u001b[0;31m     \u001b[0mcos_sim\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mcosine_similarity\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mc_vector\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mq_vector\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mc_vector\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mc_vectors\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     22\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m     \u001b[0mcombined\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcandidates\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcos_sim\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mq_positions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-104-f9a9ad3f3957>\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     19\u001b[0m     \u001b[0mc_vectors\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mquestion_to_vec\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0membeddings\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdim\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mc\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mcandidates\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 21\u001b[0;31m     \u001b[0mcos_sim\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mcosine_similarity\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mc_vector\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mq_vector\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mc_vector\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mc_vectors\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     22\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m     \u001b[0mcombined\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcandidates\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcos_sim\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mq_positions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/python3_rl/lib/python3.6/site-packages/sklearn/metrics/pairwise.py\u001b[0m in \u001b[0;36mcosine_similarity\u001b[0;34m(X, Y, dense_output)\u001b[0m\n\u001b[1;32m    900\u001b[0m         \u001b[0mY_normalized\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mX_normalized\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    901\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 902\u001b[0;31m         \u001b[0mY_normalized\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnormalize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mY\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    903\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    904\u001b[0m     K = safe_sparse_dot(X_normalized, Y_normalized.T,\n",
      "\u001b[0;32m~/anaconda3/envs/python3_rl/lib/python3.6/site-packages/sklearn/preprocessing/data.py\u001b[0m in \u001b[0;36mnormalize\u001b[0;34m(X, norm, axis, copy, return_norm)\u001b[0m\n\u001b[1;32m   1552\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1553\u001b[0m     X = check_array(X, sparse_format, copy=copy,\n\u001b[0;32m-> 1554\u001b[0;31m                     estimator='the normalize function', dtype=FLOAT_DTYPES)\n\u001b[0m\u001b[1;32m   1555\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0maxis\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1556\u001b[0m         \u001b[0mX\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mT\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/python3_rl/lib/python3.6/site-packages/sklearn/utils/validation.py\u001b[0m in \u001b[0;36mcheck_array\u001b[0;34m(array, accept_sparse, accept_large_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, ensure_min_samples, ensure_min_features, warn_on_dtype, estimator)\u001b[0m\n\u001b[1;32m    522\u001b[0m         \u001b[0;31m# thereby passing the test made in the lines following the scope\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    523\u001b[0m         \u001b[0;31m# of warnings context manager.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 524\u001b[0;31m         \u001b[0;32mwith\u001b[0m \u001b[0mwarnings\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcatch_warnings\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    525\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    526\u001b[0m                 \u001b[0mwarnings\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msimplefilter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'error'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mComplexWarning\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/python3_rl/lib/python3.6/warnings.py\u001b[0m in \u001b[0;36m__enter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    447\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0;34m\"%s(%s)\"\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\", \"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    448\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 449\u001b[0;31m     \u001b[0;32mdef\u001b[0m \u001b[0m__enter__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    450\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_entered\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    451\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mRuntimeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Cannot enter %r twice\"\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "wv_ranking = []\n",
    "for line in validation:\n",
    "    q, *ex = line\n",
    "    ranks = rank_candidates(q, ex, wv_embeddings)\n",
    "    wv_ranking.append([r[0] for r in ranks].index(0) + 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DCG@   1: 0.270 | Hits@   1: 0.270\n",
      "DCG@   5: 0.334 | Hits@   5: 0.391\n",
      "DCG@  10: 0.354 | Hits@  10: 0.455\n",
      "DCG@ 100: 0.410 | Hits@ 100: 0.739\n",
      "DCG@ 500: 0.445 | Hits@ 500: 1.000\n",
      "DCG@1000: 0.445 | Hits@1000: 1.000\n"
     ]
    }
   ],
   "source": [
    "for k in [1, 5, 10, 100, 500, 1000]:\n",
    "    print(\"DCG@%4d: %.3f | Hits@%4d: %.3f\" % (k, dcg_score(wv_ranking, k), k, hits_count(wv_ranking, k)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you did all the steps correctly, you should be frustrated by the received results. Let's try to understand why the quality is so low. First of all, when you work with some data it is necessary to have an idea how the data looks like. Print several questions from the data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "How to print a binary heap tree without recursion? How do you best convert a recursive function to an iterative one? How can i use ng-model with directive in angular js flash: drawing and erasing\n",
      "How to start PhoneStateListener programmatically? PhoneStateListener and service Java cast object[] to model WCF and What does this mean?\n",
      "jQuery: Show a div2 when mousenter over div1 is over when hover on div1 depenting on if it is on div2 or not it should act differently How to run selenium in google app engine/cloud? Python Comparing two lists of strings for similarities\n"
     ]
    }
   ],
   "source": [
    "for line in validation[:3]:\n",
    "    q, *examples = line\n",
    "    print(q, *examples[:3])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see, we deal with the raw data. It means that we have many punctuation marks, special characters and unlowercased letters. In our case, it could lead to the situation where we can't find some embeddings, e.g. for the word \"grid?\". \n",
    "\n",
    "To solve this problem you should use the functions *text_prepare* from the previous assignments to prepare the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "from util import text_prepare"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now transform all the questions from the validation set:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "prepared_validation = []\n",
    "for line in validation:\n",
    "    prepared_validation.append([text_prepare(q) for q in line])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's evaluate the approach again after the preparation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-154-0beac4f4d767>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mline\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mprepared_validation\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0mq\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0mex\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mline\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m     \u001b[0mranks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrank_candidates\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mq\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mex\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwv_embeddings\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m     \u001b[0mwv_prepared_ranking\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mr\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mr\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mranks\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-152-84c71643c78a>\u001b[0m in \u001b[0;36mrank_candidates\u001b[0;34m(question, candidates, embeddings, dim)\u001b[0m\n\u001b[1;32m     17\u001b[0m     \u001b[0mq_positions\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcandidates\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 19\u001b[0;31m     \u001b[0mcombined\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msorted\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mq_positions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcandidates\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkey\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mlambda\u001b[0m \u001b[0mp\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0mcosine_similarity\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mquestion_to_vec\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mp\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0membeddings\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mdim\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mq_vector\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     20\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mcombined\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-152-84c71643c78a>\u001b[0m in \u001b[0;36m<lambda>\u001b[0;34m(p)\u001b[0m\n\u001b[1;32m     17\u001b[0m     \u001b[0mq_positions\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcandidates\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 19\u001b[0;31m     \u001b[0mcombined\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msorted\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mq_positions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcandidates\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkey\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mlambda\u001b[0m \u001b[0mp\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0mcosine_similarity\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mquestion_to_vec\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mp\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0membeddings\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mdim\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mq_vector\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     20\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mcombined\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/python3_rl/lib/python3.6/site-packages/sklearn/metrics/pairwise.py\u001b[0m in \u001b[0;36mcosine_similarity\u001b[0;34m(X, Y, dense_output)\u001b[0m\n\u001b[1;32m    894\u001b[0m     \u001b[0;31m# to avoid recursive import\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    895\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 896\u001b[0;31m     \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mY\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcheck_pairwise_arrays\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mY\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    897\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    898\u001b[0m     \u001b[0mX_normalized\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnormalize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/python3_rl/lib/python3.6/site-packages/sklearn/metrics/pairwise.py\u001b[0m in \u001b[0;36mcheck_pairwise_arrays\u001b[0;34m(X, Y, precomputed, dtype)\u001b[0m\n\u001b[1;32m    111\u001b[0m                         warn_on_dtype=warn_on_dtype, estimator=estimator)\n\u001b[1;32m    112\u001b[0m         Y = check_array(Y, accept_sparse='csr', dtype=dtype,\n\u001b[0;32m--> 113\u001b[0;31m                         warn_on_dtype=warn_on_dtype, estimator=estimator)\n\u001b[0m\u001b[1;32m    114\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    115\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mprecomputed\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/python3_rl/lib/python3.6/site-packages/sklearn/utils/validation.py\u001b[0m in \u001b[0;36mcheck_array\u001b[0;34m(array, accept_sparse, accept_large_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, ensure_min_samples, ensure_min_features, warn_on_dtype, estimator)\u001b[0m\n\u001b[1;32m    522\u001b[0m         \u001b[0;31m# thereby passing the test made in the lines following the scope\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    523\u001b[0m         \u001b[0;31m# of warnings context manager.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 524\u001b[0;31m         \u001b[0;32mwith\u001b[0m \u001b[0mwarnings\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcatch_warnings\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    525\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    526\u001b[0m                 \u001b[0mwarnings\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msimplefilter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'error'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mComplexWarning\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/python3_rl/lib/python3.6/warnings.py\u001b[0m in \u001b[0;36m__enter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    447\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0;34m\"%s(%s)\"\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\", \"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    448\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 449\u001b[0;31m     \u001b[0;32mdef\u001b[0m \u001b[0m__enter__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    450\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_entered\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    451\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mRuntimeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Cannot enter %r twice\"\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "wv_prepared_ranking = []\n",
    "for line in prepared_validation:\n",
    "    q, *ex = line\n",
    "    ranks = rank_candidates(q, ex, wv_embeddings)\n",
    "    wv_prepared_ranking.append([r[0] for r in ranks].index(0) + 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DCG@   1: 0.311 | Hits@   1: 0.311\n",
      "DCG@   5: 0.381 | Hits@   5: 0.444\n",
      "DCG@  10: 0.397 | Hits@  10: 0.495\n",
      "DCG@ 100: 0.431 | Hits@ 100: 0.662\n",
      "DCG@ 500: 0.453 | Hits@ 500: 0.835\n",
      "DCG@1000: 0.470 | Hits@1000: 1.000\n"
     ]
    }
   ],
   "source": [
    "for k in [1, 5, 10, 100, 500, 1000]:\n",
    "    print(\"DCG@%4d: %.3f | Hits@%4d: %.3f\" % (k, dcg_score(wv_prepared_ranking, k), \n",
    "                                              k, hits_count(wv_prepared_ranking, k)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, prepare also train and test data, because you will need it in the future:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_file(in_, out_):\n",
    "    out = open(out_, 'w')\n",
    "    for line in open(in_, encoding='utf8'):\n",
    "        line = line.strip().split('\\t')\n",
    "        new_line = [text_prepare(q) for q in line]\n",
    "        print(*new_line, sep='\\t', file=out)\n",
    "    out.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test_embeddings.tsv  test.tsv  train.tsv  validation.tsv\r\n"
     ]
    }
   ],
   "source": [
    "!ls ./data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [],
   "source": [
    "######################################\n",
    "######### YOUR CODE HERE #############\n",
    "######################################\n",
    "\n",
    "prepare_file('data/train.tsv', 'data/train_prepared.tsv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [],
   "source": [
    "prepare_file('data/test.tsv', 'data/test_prepared.tsv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Task 4 (W2VTokenizedRanks).** For each question from prepared *test.tsv* submit the ranks of the candidates to earn the points. The calculations should take about 3-5 minutes. Pay attention that the function *rank_candidates* returns a ranking, while in this case you should find a position in this ranking. Ranks should start with 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from util import matrix_to_string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "w2v_ranks_results = []\n",
    "prepared_test_data = ######### YOUR CODE HERE #############\n",
    "for line in open(prepared_test_data):\n",
    "    q, *ex = line.strip().split('\\t')\n",
    "    ranks = rank_candidates(q, ex, wv_embeddings, 300)\n",
    "    ranked_candidates = [r[0] for r in ranks]\n",
    "    w2v_ranks_results.append([ranked_candidates.index(i) + 1 for i in range(len(ranked_candidates))])\n",
    "    \n",
    "grader.submit_tag('W2VTokenizedRanks', matrix_to_string(w2v_ranks_results))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Advanced solution: StarSpace embeddings\n",
    "\n",
    "Now you are ready to train your own word embeddings! In particular, you need to train embeddings specially for our task of duplicates detection. Unfortunately, StarSpace cannot be run on Windows and we recommend to use provided\n",
    "[docker container](https://github.com/hse-aml/natural-language-processing/blob/master/Docker-tutorial.md) or other alternatives. Don't delete results of this task because you will need it in the final project.\n",
    "\n",
    "### How it works and what's the main difference with word2vec?\n",
    "The main point in this section is that StarSpace can be trained specifically for some tasks. In contrast to word2vec model, which tries to train similar embeddings for words in similar contexts, StarSpace uses embeddings for the whole sentence (just as a sum of embeddings of words and phrases). Despite the fact that in both cases we get word embeddings as a result of the training, StarSpace embeddings are trained using some supervised data, e.g. a set of similar sentence pairs, and thus they can better suit the task.\n",
    "\n",
    "In our case, StarSpace should use two types of sentence pairs for training: \"positive\" and \"negative\". \"Positive\" examples are extracted from the train sample (duplicates, high similarity) and the \"negative\" examples are generated randomly (low similarity assumed). \n",
    "\n",
    "### How to choose the best params for the model?\n",
    "Normally, you would start with some default choice and then run extensive experiments to compare different strategies. However, we have some recommendations ready for you to save your time:\n",
    "- Be careful with choosing the suitable training mode. In this task we want to explore texts similarity which corresponds to *trainMode = 3*.\n",
    "- Use adagrad optimization (parameter *adagrad = true*).\n",
    "- Set the length of phrase equal to 1 (parameter *ngrams*), because we need embeddings only for words.\n",
    "- Don't use a large number of *epochs* (we think that 5 should be enough).\n",
    "- Try dimension *dim* equal to 100.\n",
    "- To compare embeddings usually *cosine* *similarity* is used.\n",
    "- Set *minCount* greater than 1 (for example, 2) if you don't want to get embeddings for extremely rare words.\n",
    "- Parameter *verbose = true* could show you the progress of the training process.\n",
    "- Set parameter *fileFormat* equals *labelDoc*.\n",
    "- Parameter *negSearchLimit* is responsible for a number of negative examples which is used during the training. We think that 10 will be enought for this task.\n",
    "- To increase a speed of training we recommend to set *learning rate* to 0.05."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train StarSpace embeddings for unigrams on the train dataset. You don't need to change the format of the input data. Just don't forget to use prepared version of the training data. \n",
    "\n",
    "If you follow the instruction, the training process will take about 1 hour. The size of the embeddings' dictionary should be approximately 100 000 (number of lines in the result file). If you got significantly more than this number, try to check all the instructions above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Arguments: \n",
      "lr: 0.05\n",
      "dim: 100\n",
      "epoch: 5\n",
      "maxTrainTime: 8640000\n",
      "validationPatience: 10\n",
      "saveEveryEpoch: 0\n",
      "loss: hinge\n",
      "margin: 0.05\n",
      "similarity: cosine\n",
      "maxNegSamples: 10\n",
      "negSearchLimit: 10\n",
      "batchSize: 5\n",
      "thread: 10\n",
      "minCount: 2\n",
      "minCountLabel: 1\n",
      "label: __label__\n",
      "label: __label__\n",
      "ngrams: 1\n",
      "bucket: 2000000\n",
      "adagrad: 1\n",
      "trainMode: 3\n",
      "fileFormat: labelDoc\n",
      "normalizeText: 0\n",
      "dropoutLHS: 0\n",
      "dropoutRHS: 0\n",
      "useWeight: 0\n",
      "weightSep: :\n",
      "Start to initialize starspace model.\n",
      "Build dict from input file : data/train_prepared.tsv\n",
      "Read 12M words\n",
      "Number of words in dictionary:  95058\n",
      "Number of labels in dictionary: 0\n",
      "Loading data from file : data/train_prepared.tsv\n",
      "Total number of examples loaded : 999740\n",
      "Initialized model weights. Model size :\n",
      "matrix : 95058 100\n",
      "Training epoch 0: 0.05 0.01\n",
      "Epoch: 100.0%  lr: 0.040000  loss: 0.042653  eta: 0h6m  tot: 0h1m43s  (20.0%)%  lr: 0.049960  loss: 0.256477  eta: 0h10m  tot: 0h0m0s  (0.1%)2.0%  lr: 0.049860  loss: 0.182547  eta: 0h10m  tot: 0h0m2s  (0.4%)4.1%  lr: 0.049600  loss: 0.138522  eta: 0h10m  tot: 0h0m5s  (0.8%)6.2%  lr: 0.049399  loss: 0.119949  eta: 0h9m  tot: 0h0m7s  (1.2%)7.0%  lr: 0.049279  loss: 0.115087  eta: 0h9m  tot: 0h0m8s  (1.4%)7.5%  lr: 0.049259  loss: 0.112403  eta: 0h9m  tot: 0h0m8s  (1.5%)7.7%  lr: 0.049219  loss: 0.111426  eta: 0h9m  tot: 0h0m8s  (1.5%)7.8%  lr: 0.049219  loss: 0.110907  eta: 0h9m  tot: 0h0m9s  (1.6%)8.0%  lr: 0.049209  loss: 0.109884  eta: 0h9m  tot: 0h0m9s  (1.6%)8.8%  lr: 0.049139  loss: 0.105576  eta: 0h9m  tot: 0h0m10s  (1.8%)10.2%  lr: 0.048959  loss: 0.099052  eta: 0h9m  tot: 0h0m11s  (2.0%)10.5%  lr: 0.048929  loss: 0.097761  eta: 0h9m  tot: 0h0m12s  (2.1%)11.3%  lr: 0.048869  loss: 0.096032  eta: 0h9m  tot: 0h0m12s  (2.3%)11.3%  lr: 0.048869  loss: 0.095864  eta: 0h9m  tot: 0h0m12s  (2.3%)12.2%  lr: 0.048799  loss: 0.093556  eta: 0h9m  tot: 0h0m13s  (2.4%)12.8%  lr: 0.048749  loss: 0.091600  eta: 0h9m  tot: 0h0m14s  (2.6%)13.1%  lr: 0.048699  loss: 0.091209  eta: 0h9m  tot: 0h0m14s  (2.6%)15.2%  lr: 0.048468  loss: 0.086485  eta: 0h9m  tot: 0h0m17s  (3.0%)15.6%  lr: 0.048448  loss: 0.086100  eta: 0h9m  tot: 0h0m17s  (3.1%)15.8%  lr: 0.048438  loss: 0.085642  eta: 0h9m  tot: 0h0m17s  (3.2%)15.9%  lr: 0.048408  loss: 0.085231  eta: 0h9m  tot: 0h0m17s  (3.2%)16.6%  lr: 0.048278  loss: 0.083949  eta: 0h9m  tot: 0h0m18s  (3.3%)h0m19s  (3.4%)17.4%  lr: 0.048168  loss: 0.082652  eta: 0h9m  tot: 0h0m19s  (3.5%)19.2%  lr: 0.048018  loss: 0.079817  eta: 0h8m  tot: 0h0m21s  (3.8%)m  tot: 0h0m21s  (3.9%)19.4%  lr: 0.048008  loss: 0.079332  eta: 0h8m  tot: 0h0m21s  (3.9%)20.3%  lr: 0.047968  loss: 0.077921  eta: 0h8m  tot: 0h0m22s  (4.1%)21.5%  lr: 0.047828  loss: 0.076277  eta: 0h8m  tot: 0h0m24s  (4.3%)22.1%  lr: 0.047798  loss: 0.075633  eta: 0h8m  tot: 0h0m24s  (4.4%)22.7%  lr: 0.047688  loss: 0.074993  eta: 0h8m  tot: 0h0m25s  (4.5%)22.9%  lr: 0.047688  loss: 0.074966  eta: 0h8m  tot: 0h0m25s  (4.6%)23.0%  lr: 0.047688  loss: 0.074850  eta: 0h8m  tot: 0h0m25s  (4.6%)23.0%  lr: 0.047658  loss: 0.074675  eta: 0h8m  tot: 0h0m25s  (4.6%)24.4%  lr: 0.047538  loss: 0.072508  eta: 0h8m  tot: 0h0m27s  (4.9%)24.9%  lr: 0.047488  loss: 0.072052  eta: 0h8m  tot: 0h0m27s  (5.0%)25.1%  lr: 0.047478  loss: 0.071806  eta: 0h8m  tot: 0h0m27s  (5.0%)26.3%  lr: 0.047367  loss: 0.070518  eta: 0h8m  tot: 0h0m29s  (5.3%)27.5%  lr: 0.047287  loss: 0.069132  eta: 0h8m  tot: 0h0m30s  (5.5%)30.2%  lr: 0.047077  loss: 0.066896  eta: 0h8m  tot: 0h0m33s  (6.0%)31.6%  lr: 0.046987  loss: 0.065701  eta: 0h8m  tot: 0h0m34s  (6.3%)31.9%  lr: 0.046987  loss: 0.065548  eta: 0h8m  tot: 0h0m34s  (6.4%)32.0%  lr: 0.046987  loss: 0.065432  eta: 0h8m  tot: 0h0m35s  (6.4%)32.7%  lr: 0.046957  loss: 0.065073  eta: 0h8m  tot: 0h0m35s  (6.5%)34.5%  lr: 0.046737  loss: 0.063860  eta: 0h8m  tot: 0h0m37s  (6.9%)34.7%  lr: 0.046727  loss: 0.063617  eta: 0h8m  tot: 0h0m38s  (6.9%)38s  (7.1%)35.5%  lr: 0.046627  loss: 0.063232  eta: 0h8m  tot: 0h0m39s  (7.1%)35.8%  lr: 0.046567  loss: 0.063055  eta: 0h8m  tot: 0h0m39s  (7.2%)36.5%  lr: 0.046497  loss: 0.062600  eta: 0h8m  tot: 0h0m40s  (7.3%)38.0%  lr: 0.046316  loss: 0.061670  eta: 0h8m  tot: 0h0m41s  (7.6%)38.5%  lr: 0.046246  loss: 0.061593  eta: 0h8m  tot: 0h0m42s  (7.7%)38.7%  lr: 0.046226  loss: 0.061467  eta: 0h8m  tot: 0h0m42s  (7.7%)38.9%  lr: 0.046226  loss: 0.061438  eta: 0h8m  tot: 0h0m42s  (7.8%)40.1%  lr: 0.046176  loss: 0.060660  eta: 0h8m  tot: 0h0m43s  (8.0%)42.3%  lr: 0.045876  loss: 0.059356  eta: 0h8m  tot: 0h0m46s  (8.5%)42.5%  lr: 0.045876  loss: 0.059245  eta: 0h8m  tot: 0h0m46s  (8.5%)42.8%  lr: 0.045826  loss: 0.059073  eta: 0h8m  tot: 0h0m46s  (8.6%)43.2%  lr: 0.045776  loss: 0.058897  eta: 0h8m  tot: 0h0m47s  (8.6%)43.5%  lr: 0.045746  loss: 0.058732  eta: 0h8m  tot: 0h0m47s  (8.7%)43.9%  lr: 0.045726  loss: 0.058540  eta: 0h8m  tot: 0h0m47s  (8.8%)44.2%  lr: 0.045716  loss: 0.058414  eta: 0h8m  tot: 0h0m48s  (8.8%)44.7%  lr: 0.045646  loss: 0.058115  eta: 0h8m  tot: 0h0m48s  (8.9%)45.3%  lr: 0.045606  loss: 0.057844  eta: 0h8m  tot: 0h0m49s  (9.1%)45.7%  lr: 0.045556  loss: 0.057746  eta: 0h8m  tot: 0h0m49s  (9.1%)46.3%  lr: 0.045516  loss: 0.057527  eta: 0h8m  tot: 0h0m50s  (9.3%)47.8%  lr: 0.045295  loss: 0.056785  eta: 0h8m  tot: 0h0m51s  (9.6%)47.9%  lr: 0.045265  loss: 0.056722  eta: 0h8m  tot: 0h0m51s  (9.6%)48.5%  lr: 0.045205  loss: 0.056479  eta: 0h8m  tot: 0h0m52s  (9.7%)48.8%  lr: 0.045165  loss: 0.056413  eta: 0h8m  tot: 0h0m52s  (9.8%)49.6%  lr: 0.045105  loss: 0.055988  eta: 0h8m  tot: 0h0m53s  (9.9%)51.8%  lr: 0.044865  loss: 0.055210  eta: 0h8m  tot: 0h0m56s  (10.4%)51.9%  lr: 0.044855  loss: 0.055086  eta: 0h8m  tot: 0h0m56s  (10.4%)52.2%  lr: 0.044845  loss: 0.055027  eta: 0h8m  tot: 0h0m56s  (10.4%)52.8%  lr: 0.044825  loss: 0.054792  eta: 0h8m  tot: 0h0m57s  (10.6%)53.5%  lr: 0.044745  loss: 0.054532  eta: 0h8m  tot: 0h0m57s  (10.7%)57.9%  lr: 0.044284  loss: 0.052961  eta: 0h7m  tot: 0h1m2s  (11.6%)58.2%  lr: 0.044224  loss: 0.052820  eta: 0h7m  tot: 0h1m2s  (11.6%)m  tot: 0h1m4s  (12.0%)60.9%  lr: 0.043924  loss: 0.051938  eta: 0h7m  tot: 0h1m5s  (12.2%)61.5%  lr: 0.043854  loss: 0.051695  eta: 0h7m  tot: 0h1m6s  (12.3%)61.9%  lr: 0.043764  loss: 0.051598  eta: 0h7m  tot: 0h1m6s  (12.4%)62.0%  lr: 0.043744  loss: 0.051564  eta: 0h7m  tot: 0h1m6s  (12.4%)62.2%  lr: 0.043734  loss: 0.051503  eta: 0h7m  tot: 0h1m6s  (12.4%)64.6%  lr: 0.043474  loss: 0.050625  eta: 0h7m  tot: 0h1m9s  (12.9%)64.9%  lr: 0.043424  loss: 0.050496  eta: 0h7m  tot: 0h1m9s  (13.0%)65.4%  lr: 0.043373  loss: 0.050378  eta: 0h7m  tot: 0h1m10s  (13.1%)66.3%  lr: 0.043273  loss: 0.050082  eta: 0h7m  tot: 0h1m11s  (13.3%)66.5%  lr: 0.043223  loss: 0.050009  eta: 0h7m  tot: 0h1m11s  (13.3%)67.2%  lr: 0.043183  loss: 0.049827  eta: 0h7m  tot: 0h1m12s  (13.4%)69.6%  lr: 0.042943  loss: 0.049049  eta: 0h7m  tot: 0h1m14s  (13.9%)71.6%  lr: 0.042753  loss: 0.048577  eta: 0h7m  tot: 0h1m16s  (14.3%)72.7%  lr: 0.042653  loss: 0.048228  eta: 0h7m  tot: 0h1m18s  (14.5%)72.9%  lr: 0.042643  loss: 0.048185  eta: 0h7m  tot: 0h1m18s  (14.6%)73.6%  lr: 0.042603  loss: 0.047943  eta: 0h7m  tot: 0h1m18s  (14.7%)74.2%  lr: 0.042563  loss: 0.047826  eta: 0h7m  tot: 0h1m19s  (14.8%)75.3%  lr: 0.042443  loss: 0.047511  eta: 0h7m  tot: 0h1m20s  (15.1%)75.7%  lr: 0.042383  loss: 0.047416  eta: 0h7m  tot: 0h1m20s  (15.1%)76.5%  lr: 0.042242  loss: 0.047235  eta: 0h7m  tot: 0h1m21s  (15.3%)76.7%  lr: 0.042212  loss: 0.047170  eta: 0h7m  tot: 0h1m21s  (15.3%)78.8%  lr: 0.042012  loss: 0.046679  eta: 0h7m  tot: 0h1m24s  (15.8%)79.2%  lr: 0.041992  loss: 0.046541  eta: 0h7m  tot: 0h1m24s  (15.8%)79.6%  lr: 0.041952  loss: 0.046497  eta: 0h7m  tot: 0h1m24s  (15.9%)80.7%  lr: 0.041862  loss: 0.046322  eta: 0h7m  tot: 0h1m25s  (16.1%)81.7%  lr: 0.041792  loss: 0.046081  eta: 0h7m  tot: 0h1m26s  (16.3%)83.3%  lr: 0.041592  loss: 0.045741  eta: 0h7m  tot: 0h1m28s  (16.7%)84.1%  lr: 0.041422  loss: 0.045529  eta: 0h7m  tot: 0h1m29s  (16.8%)84.2%  lr: 0.041392  loss: 0.045519  eta: 0h7m  tot: 0h1m29s  (16.8%)84.4%  lr: 0.041341  loss: 0.045464  eta: 0h7m  tot: 0h1m29s  (16.9%)84.5%  lr: 0.041341  loss: 0.045454  eta: 0h7m  tot: 0h1m29s  (16.9%)85.8%  lr: 0.041171  loss: 0.045219  eta: 0h7m  tot: 0h1m31s  (17.2%)86.1%  lr: 0.041141  loss: 0.045187  eta: 0h7m  tot: 0h1m31s  (17.2%)86.9%  lr: 0.041021  loss: 0.045025  eta: 0h7m  tot: 0h1m32s  (17.4%)m  tot: 0h1m34s  (17.6%)88.5%  lr: 0.040771  loss: 0.044738  eta: 0h7m  tot: 0h1m34s  (17.7%)89.7%  lr: 0.040631  loss: 0.044514  eta: 0h7m  tot: 0h1m35s  (17.9%)m  tot: 0h1m36s  (18.0%)90.1%  lr: 0.040581  loss: 0.044408  eta: 0h7m  tot: 0h1m36s  (18.0%)90.2%  lr: 0.040551  loss: 0.044387  eta: 0h7m  tot: 0h1m36s  (18.0%)91.6%  lr: 0.040461  loss: 0.044195  eta: 0h7m  tot: 0h1m37s  (18.3%)\n",
      " ---+++                Epoch    0 Train error : 0.04330192 +++--- ☃\n",
      "Training epoch 1: 0.04 0.01\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 100.0%  lr: 0.030000  loss: 0.012927  eta: 0h4m  tot: 0h3m17s  (40.0%)3%  lr: 0.039890  loss: 0.011422  eta: 0h5m  tot: 0h1m44s  (20.3%)2.1%  lr: 0.039810  loss: 0.011565  eta: 0h5m  tot: 0h1m45s  (20.4%)2.3%  lr: 0.039810  loss: 0.012541  eta: 0h5m  tot: 0h1m45s  (20.5%)%  lr: 0.039309  loss: 0.012746  eta: 0h6m  tot: 0h1m48s  (21.1%)7.1%  lr: 0.039119  loss: 0.012701  eta: 0h6m  tot: 0h1m50s  (21.4%)7.3%  lr: 0.039099  loss: 0.012755  eta: 0h6m  tot: 0h1m50s  (21.5%)7.4%  lr: 0.039079  loss: 0.012877  eta: 0h6m  tot: 0h1m50s  (21.5%)7.6%  lr: 0.039079  loss: 0.012946  eta: 0h6m  tot: 0h1m50s  (21.5%)8.1%  lr: 0.039009  loss: 0.012866  eta: 0h6m  tot: 0h1m51s  (21.6%)8.6%  lr: 0.038969  loss: 0.013010  eta: 0h6m  tot: 0h1m51s  (21.7%)10.6%  lr: 0.038779  loss: 0.013238  eta: 0h6m  tot: 0h1m53s  (22.1%)13.3%  lr: 0.038479  loss: 0.013134  eta: 0h6m  tot: 0h1m55s  (22.7%)14.5%  lr: 0.038328  loss: 0.013486  eta: 0h5m  tot: 0h1m56s  (22.9%)14.9%  lr: 0.038318  loss: 0.013452  eta: 0h5m  tot: 0h1m57s  (23.0%)16.0%  lr: 0.038198  loss: 0.013258  eta: 0h5m  tot: 0h1m58s  (23.2%)17.0%  lr: 0.038138  loss: 0.013156  eta: 0h6m  tot: 0h1m59s  (23.4%)18.5%  lr: 0.038048  loss: 0.013088  eta: 0h5m  tot: 0h2m0s  (23.7%)18.7%  lr: 0.038018  loss: 0.013074  eta: 0h5m  tot: 0h2m0s  (23.7%)19.4%  lr: 0.037938  loss: 0.012968  eta: 0h5m  tot: 0h2m1s  (23.9%)20.5%  lr: 0.037828  loss: 0.012880  eta: 0h5m  tot: 0h2m2s  (24.1%)22.1%  lr: 0.037768  loss: 0.012839  eta: 0h5m  tot: 0h2m3s  (24.4%)22.6%  lr: 0.037718  loss: 0.012829  eta: 0h5m  tot: 0h2m4s  (24.5%)24.0%  lr: 0.037548  loss: 0.012865  eta: 0h5m  tot: 0h2m5s  (24.8%)24.1%  lr: 0.037548  loss: 0.012847  eta: 0h5m  tot: 0h2m5s  (24.8%)24.3%  lr: 0.037548  loss: 0.012847  eta: 0h5m  tot: 0h2m6s  (24.9%)25.3%  lr: 0.037447  loss: 0.012866  eta: 0h5m  tot: 0h2m7s  (25.1%)25.6%  lr: 0.037427  loss: 0.012831  eta: 0h5m  tot: 0h2m7s  (25.1%)25.8%  lr: 0.037417  loss: 0.012839  eta: 0h5m  tot: 0h2m7s  (25.2%)26.7%  lr: 0.037327  loss: 0.012964  eta: 0h5m  tot: 0h2m8s  (25.3%)m8s  (25.5%)28.5%  lr: 0.037177  loss: 0.013005  eta: 0h5m  tot: 0h2m10s  (25.7%)28.6%  lr: 0.037137  loss: 0.012974  eta: 0h5m  tot: 0h2m10s  (25.7%)29.1%  lr: 0.037067  loss: 0.012972  eta: 0h5m  tot: 0h2m10s  (25.8%)30.4%  lr: 0.036937  loss: 0.012957  eta: 0h5m  tot: 0h2m11s  (26.1%)30.6%  lr: 0.036937  loss: 0.012942  eta: 0h5m  tot: 0h2m12s  (26.1%)30.7%  lr: 0.036927  loss: 0.012961  eta: 0h5m  tot: 0h2m12s  (26.1%)33.1%  lr: 0.036637  loss: 0.012994  eta: 0h5m  tot: 0h2m14s  (26.6%)34.0%  lr: 0.036487  loss: 0.013059  eta: 0h5m  tot: 0h2m15s  (26.8%)36.7%  lr: 0.036296  loss: 0.013157  eta: 0h5m  tot: 0h2m18s  (27.3%)37.7%  lr: 0.036196  loss: 0.013205  eta: 0h5m  tot: 0h2m19s  (27.5%)38.2%  lr: 0.036156  loss: 0.013175  eta: 0h5m  tot: 0h2m19s  (27.6%)39.1%  lr: 0.036076  loss: 0.013178  eta: 0h5m  tot: 0h2m20s  (27.8%)39.8%  lr: 0.035966  loss: 0.013082  eta: 0h5m  tot: 0h2m21s  (28.0%)41.0%  lr: 0.035826  loss: 0.013011  eta: 0h5m  tot: 0h2m22s  (28.2%)41.3%  lr: 0.035766  loss: 0.013047  eta: 0h5m  tot: 0h2m22s  (28.3%)43.7%  lr: 0.035375  loss: 0.013159  eta: 0h5m  tot: 0h2m25s  (28.7%)44.6%  lr: 0.035285  loss: 0.013217  eta: 0h5m  tot: 0h2m25s  (28.9%)44.7%  lr: 0.035265  loss: 0.013209  eta: 0h5m  tot: 0h2m25s  (28.9%)48.3%  lr: 0.034975  loss: 0.013192  eta: 0h5m  tot: 0h2m29s  (29.7%)48.5%  lr: 0.034955  loss: 0.013237  eta: 0h5m  tot: 0h2m29s  (29.7%)49.1%  lr: 0.034925  loss: 0.013229  eta: 0h5m  tot: 0h2m30s  (29.8%)51.6%  lr: 0.034675  loss: 0.013269  eta: 0h5m  tot: 0h2m32s  (30.3%)51.8%  lr: 0.034655  loss: 0.013250  eta: 0h5m  tot: 0h2m32s  (30.4%)52.1%  lr: 0.034615  loss: 0.013203  eta: 0h5m  tot: 0h2m32s  (30.4%)52.2%  lr: 0.034615  loss: 0.013205  eta: 0h5m  tot: 0h2m33s  (30.4%)52.7%  lr: 0.034545  loss: 0.013198  eta: 0h5m  tot: 0h2m33s  (30.5%)53.7%  lr: 0.034445  loss: 0.013178  eta: 0h5m  tot: 0h2m34s  (30.7%)54.3%  lr: 0.034374  loss: 0.013194  eta: 0h5m  tot: 0h2m34s  (30.9%)55.1%  lr: 0.034284  loss: 0.013209  eta: 0h5m  tot: 0h2m35s  (31.0%)55.2%  lr: 0.034284  loss: 0.013207  eta: 0h5m  tot: 0h2m35s  (31.0%)55.6%  lr: 0.034264  loss: 0.013214  eta: 0h5m  tot: 0h2m36s  (31.1%)56.1%  lr: 0.034204  loss: 0.013186  eta: 0h5m  tot: 0h2m36s  (31.2%)m  tot: 0h2m37s  (31.3%)56.5%  lr: 0.034114  loss: 0.013203  eta: 0h5m  tot: 0h2m37s  (31.3%)%  lr: 0.034084  loss: 0.013195  eta: 0h5m  tot: 0h2m37s  (31.4%)59.3%  lr: 0.033884  loss: 0.013281  eta: 0h5m  tot: 0h2m39s  (31.9%)60.2%  lr: 0.033794  loss: 0.013284  eta: 0h5m  tot: 0h2m40s  (32.0%)61.6%  lr: 0.033644  loss: 0.013226  eta: 0h5m  tot: 0h2m42s  (32.3%)63.0%  lr: 0.033434  loss: 0.013221  eta: 0h5m  tot: 0h2m43s  (32.6%)63.9%  lr: 0.033333  loss: 0.013228  eta: 0h5m  tot: 0h2m44s  (32.8%)64.0%  lr: 0.033313  loss: 0.013219  eta: 0h5m  tot: 0h2m44s  (32.8%)66.3%  lr: 0.033083  loss: 0.013240  eta: 0h5m  tot: 0h2m46s  (33.3%)5m  tot: 0h2m48s  (33.6%)5m  tot: 0h2m48s  (33.6%)68.5%  lr: 0.032843  loss: 0.013164  eta: 0h5m  tot: 0h2m48s  (33.7%)70.2%  lr: 0.032683  loss: 0.013163  eta: 0h5m  tot: 0h2m50s  (34.0%)70.9%  lr: 0.032613  loss: 0.013161  eta: 0h5m  tot: 0h2m50s  (34.2%)72.4%  lr: 0.032493  loss: 0.013148  eta: 0h5m  tot: 0h2m52s  (34.5%)73.7%  lr: 0.032403  loss: 0.013163  eta: 0h5m  tot: 0h2m53s  (34.7%)74.4%  lr: 0.032342  loss: 0.013125  eta: 0h5m  tot: 0h2m54s  (34.9%)74.4%  lr: 0.032332  loss: 0.013130  eta: 0h5m  tot: 0h2m54s  (34.9%)75.1%  lr: 0.032242  loss: 0.013130  eta: 0h5m  tot: 0h2m54s  (35.0%)75.5%  lr: 0.032182  loss: 0.013106  eta: 0h5m  tot: 0h2m55s  (35.1%)76.7%  lr: 0.032032  loss: 0.013097  eta: 0h5m  tot: 0h2m56s  (35.3%)77.6%  lr: 0.031922  loss: 0.013082  eta: 0h5m  tot: 0h2m57s  (35.5%)77.8%  lr: 0.031892  loss: 0.013084  eta: 0h5m  tot: 0h2m57s  (35.6%)78.0%  lr: 0.031852  loss: 0.013085  eta: 0h5m  tot: 0h2m57s  (35.6%)78.2%  lr: 0.031812  loss: 0.013095  eta: 0h5m  tot: 0h2m58s  (35.6%)80.7%  lr: 0.031582  loss: 0.013077  eta: 0h5m  tot: 0h3m0s  (36.1%)80.8%  lr: 0.031582  loss: 0.013065  eta: 0h5m  tot: 0h3m0s  (36.2%)81.0%  lr: 0.031572  loss: 0.013043  eta: 0h5m  tot: 0h3m0s  (36.2%)m  tot: 0h3m1s  (36.4%)  tot: 0h3m3s  (36.7%)84.7%  lr: 0.031171  loss: 0.013059  eta: 0h5m  tot: 0h3m4s  (36.9%)85.6%  lr: 0.031081  loss: 0.013082  eta: 0h5m  tot: 0h3m5s  (37.1%)86.3%  lr: 0.031031  loss: 0.013079  eta: 0h5m  tot: 0h3m6s  (37.3%)87.0%  lr: 0.030981  loss: 0.013059  eta: 0h5m  tot: 0h3m6s  (37.4%)87.1%  lr: 0.030941  loss: 0.013053  eta: 0h5m  tot: 0h3m7s  (37.4%)%  lr: 0.030871  loss: 0.013054  eta: 0h4m  tot: 0h3m7s  (37.5%)88.0%  lr: 0.030851  loss: 0.013043  eta: 0h4m  tot: 0h3m7s  (37.6%)%  lr: 0.030831  loss: 0.013052  eta: 0h4m  tot: 0h3m7s  (37.6%)88.8%  lr: 0.030791  loss: 0.013049  eta: 0h4m  tot: 0h3m8s  (37.8%)89.8%  lr: 0.030711  loss: 0.013040  eta: 0h4m  tot: 0h3m9s  (38.0%)90.0%  lr: 0.030701  loss: 0.013036  eta: 0h4m  tot: 0h3m9s  (38.0%)90.7%  lr: 0.030631  loss: 0.013036  eta: 0h4m  tot: 0h3m10s  (38.1%)91.6%  lr: 0.030591  loss: 0.012999  eta: 0h4m  tot: 0h3m11s  (38.3%)91.8%  lr: 0.030551  loss: 0.012992  eta: 0h4m  tot: 0h3m11s  (38.4%)92.4%  lr: 0.030511  loss: 0.012973  eta: 0h4m  tot: 0h3m11s  (38.5%)93.1%  lr: 0.030461  loss: 0.012943  eta: 0h4m  tot: 0h3m12s  (38.6%)93.6%  lr: 0.030401  loss: 0.012936  eta: 0h4m  tot: 0h3m13s  (38.7%)93.9%  lr: 0.030401  loss: 0.012934  eta: 0h4m  tot: 0h3m13s  (38.8%)94.2%  lr: 0.030371  loss: 0.012933  eta: 0h4m  tot: 0h3m13s  (38.8%)94.9%  lr: 0.030330  loss: 0.012929  eta: 0h4m  tot: 0h3m14s  (39.0%)95.3%  lr: 0.030280  loss: 0.012922  eta: 0h4m  tot: 0h3m14s  (39.1%)95.4%  lr: 0.030250  loss: 0.012926  eta: 0h4m  tot: 0h3m14s  (39.1%)96.2%  lr: 0.030220  loss: 0.012920  eta: 0h4m  tot: 0h3m15s  (39.2%)97.0%  lr: 0.030160  loss: 0.012930  eta: 0h4m  tot: 0h3m16s  (39.4%)m  tot: 0h3m16s  (39.5%)\n",
      " ---+++                Epoch    1 Train error : 0.01320680 +++--- ☃\n",
      "Training epoch 2: 0.03 0.01\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 92.5%  lr: 0.020551  loss: 0.009669  eta: 0h3m  tot: 0h4m46s  (58.5%).3%  lr: 0.029760  loss: 0.008904  eta: 0h4m  tot: 0h3m19s  (40.5%)2.4%  lr: 0.029750  loss: 0.009002  eta: 0h4m  tot: 0h3m19s  (40.5%)2.7%  lr: 0.029710  loss: 0.009229  eta: 0h4m  tot: 0h3m19s  (40.5%)4.1%  lr: 0.029570  loss: 0.009070  eta: 0h4m  tot: 0h3m21s  (40.8%)5.0%  lr: 0.029500  loss: 0.009463  eta: 0h4m  tot: 0h3m22s  (41.0%)5.1%  lr: 0.029500  loss: 0.009400  eta: 0h4m  tot: 0h3m22s  (41.0%)6.1%  lr: 0.029409  loss: 0.009410  eta: 0h4m  tot: 0h3m23s  (41.2%)7.7%  lr: 0.029289  loss: 0.009386  eta: 0h4m  tot: 0h3m24s  (41.5%)0h3m24s  (41.5%)8.9%  lr: 0.029179  loss: 0.009574  eta: 0h4m  tot: 0h3m25s  (41.8%)10.6%  lr: 0.029009  loss: 0.009616  eta: 0h4m  tot: 0h3m27s  (42.1%)13.3%  lr: 0.028729  loss: 0.009602  eta: 0h4m  tot: 0h3m29s  (42.7%)13.4%  lr: 0.028729  loss: 0.009601  eta: 0h4m  tot: 0h3m29s  (42.7%)14.5%  lr: 0.028609  loss: 0.009598  eta: 0h4m  tot: 0h3m31s  (42.9%)15.6%  lr: 0.028519  loss: 0.009645  eta: 0h4m  tot: 0h3m32s  (43.1%)16.2%  lr: 0.028448  loss: 0.009689  eta: 0h4m  tot: 0h3m32s  (43.2%)16.9%  lr: 0.028368  loss: 0.009709  eta: 0h4m  tot: 0h3m33s  (43.4%)17.7%  lr: 0.028308  loss: 0.009722  eta: 0h4m  tot: 0h3m34s  (43.5%)18.7%  lr: 0.028158  loss: 0.009730  eta: 0h4m  tot: 0h3m35s  (43.7%)19.8%  lr: 0.028058  loss: 0.009778  eta: 0h4m  tot: 0h3m36s  (44.0%)20.9%  lr: 0.027968  loss: 0.009744  eta: 0h4m  tot: 0h3m37s  (44.2%)21.0%  lr: 0.027948  loss: 0.009746  eta: 0h4m  tot: 0h3m37s  (44.2%)23.1%  lr: 0.027638  loss: 0.009822  eta: 0h4m  tot: 0h3m39s  (44.6%)23.2%  lr: 0.027638  loss: 0.009814  eta: 0h4m  tot: 0h3m39s  (44.6%)24.3%  lr: 0.027518  loss: 0.009743  eta: 0h4m  tot: 0h3m40s  (44.9%)24.7%  lr: 0.027468  loss: 0.009716  eta: 0h4m  tot: 0h3m41s  (44.9%)25.7%  lr: 0.027357  loss: 0.009774  eta: 0h4m  tot: 0h3m42s  (45.1%)26.4%  lr: 0.027307  loss: 0.009678  eta: 0h4m  tot: 0h3m42s  (45.3%)26.8%  lr: 0.027247  loss: 0.009668  eta: 0h4m  tot: 0h3m43s  (45.4%)27.2%  lr: 0.027187  loss: 0.009673  eta: 0h4m  tot: 0h3m43s  (45.4%)27.5%  lr: 0.027157  loss: 0.009706  eta: 0h4m  tot: 0h3m43s  (45.5%)27.7%  lr: 0.027117  loss: 0.009730  eta: 0h4m  tot: 0h3m43s  (45.5%)29.3%  lr: 0.026967  loss: 0.009749  eta: 0h4m  tot: 0h3m45s  (45.9%)29.5%  lr: 0.026957  loss: 0.009745  eta: 0h4m  tot: 0h3m45s  (45.9%)30.5%  lr: 0.026897  loss: 0.009747  eta: 0h4m  tot: 0h3m46s  (46.1%)30.8%  lr: 0.026857  loss: 0.009757  eta: 0h4m  tot: 0h3m46s  (46.2%)30.9%  lr: 0.026857  loss: 0.009773  eta: 0h4m  tot: 0h3m46s  (46.2%)%  lr: 0.026507  loss: 0.009567  eta: 0h4m  tot: 0h3m49s  (46.7%)33.8%  lr: 0.026487  loss: 0.009623  eta: 0h4m  tot: 0h3m49s  (46.8%)35.0%  lr: 0.026346  loss: 0.009575  eta: 0h4m  tot: 0h3m50s  (47.0%)35.7%  lr: 0.026306  loss: 0.009579  eta: 0h4m  tot: 0h3m51s  (47.1%)36.3%  lr: 0.026246  loss: 0.009551  eta: 0h4m  tot: 0h3m52s  (47.3%)36.4%  lr: 0.026236  loss: 0.009558  eta: 0h4m  tot: 0h3m52s  (47.3%)37.6%  lr: 0.026086  loss: 0.009518  eta: 0h4m  tot: 0h3m53s  (47.5%)37.9%  lr: 0.026036  loss: 0.009516  eta: 0h4m  tot: 0h3m53s  (47.6%)38.3%  lr: 0.025986  loss: 0.009584  eta: 0h4m  tot: 0h3m54s  (47.7%)38.5%  lr: 0.025956  loss: 0.009602  eta: 0h4m  tot: 0h3m54s  (47.7%)39.0%  lr: 0.025886  loss: 0.009668  eta: 0h4m  tot: 0h3m54s  (47.8%)39.1%  lr: 0.025886  loss: 0.009669  eta: 0h4m  tot: 0h3m54s  (47.8%)40.1%  lr: 0.025776  loss: 0.009630  eta: 0h4m  tot: 0h3m55s  (48.0%)41.7%  lr: 0.025626  loss: 0.009613  eta: 0h4m  tot: 0h3m57s  (48.3%)42.2%  lr: 0.025566  loss: 0.009597  eta: 0h4m  tot: 0h3m58s  (48.4%)42.7%  lr: 0.025536  loss: 0.009592  eta: 0h4m  tot: 0h3m58s  (48.5%)43.3%  lr: 0.025486  loss: 0.009591  eta: 0h4m  tot: 0h3m59s  (48.7%)43.8%  lr: 0.025395  loss: 0.009561  eta: 0h4m  tot: 0h3m59s  (48.8%)43.9%  lr: 0.025395  loss: 0.009559  eta: 0h4m  tot: 0h3m59s  (48.8%)44.6%  lr: 0.025315  loss: 0.009640  eta: 0h4m  tot: 0h4m0s  (48.9%)46.2%  lr: 0.025125  loss: 0.009654  eta: 0h4m  tot: 0h4m1s  (49.2%)46.3%  lr: 0.025115  loss: 0.009657  eta: 0h4m  tot: 0h4m1s  (49.3%)47.5%  lr: 0.024985  loss: 0.009653  eta: 0h4m  tot: 0h4m2s  (49.5%)47.7%  lr: 0.024975  loss: 0.009653  eta: 0h4m  tot: 0h4m3s  (49.5%)49.1%  lr: 0.024865  loss: 0.009650  eta: 0h3m  tot: 0h4m4s  (49.8%)49.2%  lr: 0.024845  loss: 0.009650  eta: 0h3m  tot: 0h4m4s  (49.8%)49.7%  lr: 0.024795  loss: 0.009720  eta: 0h3m  tot: 0h4m4s  (49.9%)51.0%  lr: 0.024695  loss: 0.009710  eta: 0h3m  tot: 0h4m6s  (50.2%)51.2%  lr: 0.024685  loss: 0.009723  eta: 0h3m  tot: 0h4m6s  (50.2%)51.7%  lr: 0.024635  loss: 0.009730  eta: 0h3m  tot: 0h4m6s  (50.3%)52.6%  lr: 0.024575  loss: 0.009711  eta: 0h3m  tot: 0h4m7s  (50.5%)m  tot: 0h4m7s  (50.5%)53.6%  lr: 0.024445  loss: 0.009732  eta: 0h3m  tot: 0h4m8s  (50.7%)54.3%  lr: 0.024334  loss: 0.009724  eta: 0h3m  tot: 0h4m9s  (50.9%)54.6%  lr: 0.024304  loss: 0.009714  eta: 0h3m  tot: 0h4m9s  (50.9%)56.4%  lr: 0.024054  loss: 0.009723  eta: 0h3m  tot: 0h4m11s  (51.3%)57.1%  lr: 0.023994  loss: 0.009737  eta: 0h3m  tot: 0h4m12s  (51.4%)57.6%  lr: 0.023944  loss: 0.009723  eta: 0h3m  tot: 0h4m12s  (51.5%)57.9%  lr: 0.023894  loss: 0.009732  eta: 0h3m  tot: 0h4m13s  (51.6%)59.5%  lr: 0.023734  loss: 0.009741  eta: 0h3m  tot: 0h4m14s  (51.9%)62.3%  lr: 0.023353  loss: 0.009769  eta: 0h3m  tot: 0h4m17s  (52.5%)65.4%  lr: 0.023033  loss: 0.009778  eta: 0h3m  tot: 0h4m20s  (53.1%)65.4%  lr: 0.023033  loss: 0.009782  eta: 0h3m  tot: 0h4m21s  (53.1%)53.3%)67.0%  lr: 0.022833  loss: 0.009797  eta: 0h3m  tot: 0h4m22s  (53.4%)67.2%  lr: 0.022823  loss: 0.009795  eta: 0h3m  tot: 0h4m22s  (53.4%)67.4%  lr: 0.022823  loss: 0.009781  eta: 0h3m  tot: 0h4m23s  (53.5%)67.5%  lr: 0.022813  loss: 0.009784  eta: 0h3m  tot: 0h4m23s  (53.5%)68.3%  lr: 0.022733  loss: 0.009773  eta: 0h3m  tot: 0h4m23s  (53.7%)68.7%  lr: 0.022683  loss: 0.009767  eta: 0h3m  tot: 0h4m24s  (53.7%)%  lr: 0.022673  loss: 0.009762  eta: 0h3m  tot: 0h4m24s  (53.8%)68.9%  lr: 0.022653  loss: 0.009763  eta: 0h3m  tot: 0h4m24s  (53.8%)69.2%  lr: 0.022633  loss: 0.009778  eta: 0h3m  tot: 0h4m24s  (53.8%)70.7%  lr: 0.022513  loss: 0.009749  eta: 0h3m  tot: 0h4m26s  (54.1%)71.1%  lr: 0.022483  loss: 0.009758  eta: 0h3m  tot: 0h4m26s  (54.2%)72.5%  lr: 0.022382  loss: 0.009777  eta: 0h3m  tot: 0h4m27s  (54.5%)75.0%  lr: 0.022202  loss: 0.009760  eta: 0h3m  tot: 0h4m30s  (55.0%)75.3%  lr: 0.022172  loss: 0.009763  eta: 0h3m  tot: 0h4m30s  (55.1%)78.1%  lr: 0.021842  loss: 0.009745  eta: 0h3m  tot: 0h4m33s  (55.6%)%  lr: 0.021782  loss: 0.009722  eta: 0h3m  tot: 0h4m33s  (55.7%)79.5%  lr: 0.021682  loss: 0.009752  eta: 0h3m  tot: 0h4m34s  (55.9%)%  lr: 0.021652  loss: 0.009743  eta: 0h3m  tot: 0h4m34s  (55.9%)79.8%  lr: 0.021602  loss: 0.009743  eta: 0h3m  tot: 0h4m34s  (56.0%)80.4%  lr: 0.021532  loss: 0.009730  eta: 0h3m  tot: 0h4m35s  (56.1%)80.8%  lr: 0.021532  loss: 0.009723  eta: 0h3m  tot: 0h4m35s  (56.2%)81.2%  lr: 0.021462  loss: 0.009736  eta: 0h3m  tot: 0h4m35s  (56.2%)82.0%  lr: 0.021412  loss: 0.009717  eta: 0h3m  tot: 0h4m36s  (56.4%)82.3%  lr: 0.021412  loss: 0.009711  eta: 0h3m  tot: 0h4m36s  (56.5%)82.6%  lr: 0.021361  loss: 0.009724  eta: 0h3m  tot: 0h4m37s  (56.5%)82.7%  lr: 0.021341  loss: 0.009735  eta: 0h3m  tot: 0h4m37s  (56.5%)0h4m38s  (56.8%)84.2%  lr: 0.021221  loss: 0.009693  eta: 0h3m  tot: 0h4m38s  (56.8%)84.4%  lr: 0.021211  loss: 0.009704  eta: 0h3m  tot: 0h4m38s  (56.9%)84.7%  lr: 0.021181  loss: 0.009698  eta: 0h3m  tot: 0h4m39s  (56.9%)85.0%  lr: 0.021181  loss: 0.009689  eta: 0h3m  tot: 0h4m39s  (57.0%)85.1%  lr: 0.021181  loss: 0.009704  eta: 0h3m  tot: 0h4m39s  (57.0%)86.7%  lr: 0.021081  loss: 0.009685  eta: 0h3m  tot: 0h4m41s  (57.3%)87.7%  lr: 0.021001  loss: 0.009660  eta: 0h3m  tot: 0h4m41s  (57.5%)88.0%  lr: 0.020981  loss: 0.009665  eta: 0h3m  tot: 0h4m42s  (57.6%)89.8%  lr: 0.020791  loss: 0.009648  eta: 0h3m  tot: 0h4m43s  (58.0%)90.7%  lr: 0.020701  loss: 0.009643  eta: 0h3m  tot: 0h4m44s  (58.1%)90.7%  lr: 0.020701  loss: 0.009655  eta: 0h3m  tot: 0h4m44s  (58.1%)91.5%  lr: 0.020601  loss: 0.009654  eta: 0h3m  tot: 0h4m45s  (58.3%)92.1%  lr: 0.020591  loss: 0.009640  eta: 0h3m  tot: 0h4m45s  (58.4%)92.5%  lr: 0.020541  loss: 0.009668  eta: 0h3m  tot: 0h4m46s  (58.5%)Epoch: 100.0%  lr: 0.020000  loss: 0.009670  eta: 0h3m  tot: 0h4m51s  (60.0%)3.4%  lr: 0.020441  loss: 0.009675  eta: 0h3m  tot: 0h4m47s  (58.7%)93.6%  lr: 0.020441  loss: 0.009668  eta: 0h3m  tot: 0h4m47s  (58.7%)94.2%  lr: 0.020371  loss: 0.009678  eta: 0h3m  tot: 0h4m48s  (58.8%)97.2%  lr: 0.020070  loss: 0.009684  eta: 0h3m  tot: 0h4m50s  (59.4%)\n",
      " ---+++                Epoch    2 Train error : 0.00944511 +++--- ☃\n",
      "Training epoch 3: 0.02 0.01\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 100.0%  lr: 0.010000  loss: 0.007761  eta: 0h1m  tot: 0h6m23s  (80.0%)1%  lr: 0.019950  loss: 0.009437  eta: 0h1m  tot: 0h4m52s  (60.2%)1.3%  lr: 0.019930  loss: 0.008555  eta: 0h1m  tot: 0h4m52s  (60.3%)1.8%  lr: 0.019900  loss: 0.007835  eta: 0h2m  tot: 0h4m52s  (60.4%)4.1%  lr: 0.019670  loss: 0.007891  eta: 0h2m  tot: 0h4m54s  (60.8%)4.7%  lr: 0.019600  loss: 0.007663  eta: 0h2m  tot: 0h4m55s  (60.9%)5.0%  lr: 0.019580  loss: 0.007475  eta: 0h2m  tot: 0h4m55s  (61.0%)5.9%  lr: 0.019449  loss: 0.007274  eta: 0h2m  tot: 0h4m56s  (61.2%)7.7%  lr: 0.019229  loss: 0.007320  eta: 0h2m  tot: 0h4m58s  (61.5%)7.7%  lr: 0.019229  loss: 0.007444  eta: 0h2m  tot: 0h4m58s  (61.5%)8.0%  lr: 0.019209  loss: 0.007440  eta: 0h2m  tot: 0h4m58s  (61.6%)8.6%  lr: 0.019119  loss: 0.007602  eta: 0h2m  tot: 0h4m59s  (61.7%)8.9%  lr: 0.019099  loss: 0.007557  eta: 0h2m  tot: 0h4m59s  (61.8%)9.2%  lr: 0.019049  loss: 0.007651  eta: 0h2m  tot: 0h4m59s  (61.8%)9.3%  lr: 0.019039  loss: 0.007653  eta: 0h2m  tot: 0h4m59s  (61.9%)9.5%  lr: 0.019029  loss: 0.007616  eta: 0h2m  tot: 0h4m59s  (61.9%)9.7%  lr: 0.019009  loss: 0.007702  eta: 0h2m  tot: 0h5m0s  (61.9%)9.8%  lr: 0.018989  loss: 0.007676  eta: 0h2m  tot: 0h5m0s  (62.0%)10.4%  lr: 0.018939  loss: 0.007608  eta: 0h2m  tot: 0h5m0s  (62.1%)10.7%  lr: 0.018929  loss: 0.007605  eta: 0h2m  tot: 0h5m0s  (62.1%)11.5%  lr: 0.018889  loss: 0.007571  eta: 0h2m  tot: 0h5m1s  (62.3%)13.0%  lr: 0.018769  loss: 0.007436  eta: 0h2m  tot: 0h5m3s  (62.6%)13.4%  lr: 0.018749  loss: 0.007495  eta: 0h2m  tot: 0h5m3s  (62.7%)15.1%  lr: 0.018539  loss: 0.007361  eta: 0h2m  tot: 0h5m4s  (63.0%)17.4%  lr: 0.018258  loss: 0.007545  eta: 0h2m  tot: 0h5m7s  (63.5%)23.1%  lr: 0.017638  loss: 0.007344  eta: 0h2m  tot: 0h5m12s  (64.6%)23.3%  lr: 0.017588  loss: 0.007371  eta: 0h2m  tot: 0h5m12s  (64.7%)26.0%  lr: 0.017347  loss: 0.007421  eta: 0h2m  tot: 0h5m15s  (65.2%)26.7%  lr: 0.017237  loss: 0.007409  eta: 0h2m  tot: 0h5m16s  (65.3%)27.1%  lr: 0.017157  loss: 0.007391  eta: 0h2m  tot: 0h5m16s  (65.4%)28.9%  lr: 0.016977  loss: 0.007530  eta: 0h2m  tot: 0h5m18s  (65.8%)29.8%  lr: 0.016867  loss: 0.007562  eta: 0h2m  tot: 0h5m18s  (66.0%)31.1%  lr: 0.016767  loss: 0.007562  eta: 0h2m  tot: 0h5m20s  (66.2%)31.2%  lr: 0.016757  loss: 0.007557  eta: 0h2m  tot: 0h5m20s  (66.2%)32.0%  lr: 0.016607  loss: 0.007607  eta: 0h2m  tot: 0h5m21s  (66.4%)32.7%  lr: 0.016567  loss: 0.007620  eta: 0h2m  tot: 0h5m21s  (66.5%)33.3%  lr: 0.016507  loss: 0.007625  eta: 0h2m  tot: 0h5m22s  (66.7%)34.4%  lr: 0.016376  loss: 0.007607  eta: 0h2m  tot: 0h5m23s  (66.9%)34.9%  lr: 0.016336  loss: 0.007625  eta: 0h2m  tot: 0h5m23s  (67.0%)35.0%  lr: 0.016336  loss: 0.007648  eta: 0h2m  tot: 0h5m23s  (67.0%)36.5%  lr: 0.016126  loss: 0.007685  eta: 0h2m  tot: 0h5m25s  (67.3%)37.1%  lr: 0.016066  loss: 0.007652  eta: 0h2m  tot: 0h5m25s  (67.4%)37.6%  lr: 0.016026  loss: 0.007673  eta: 0h2m  tot: 0h5m26s  (67.5%)37.8%  lr: 0.016006  loss: 0.007673  eta: 0h2m  tot: 0h5m26s  (67.6%)39.1%  lr: 0.015926  loss: 0.007661  eta: 0h2m  tot: 0h5m27s  (67.8%)39.2%  lr: 0.015916  loss: 0.007653  eta: 0h2m  tot: 0h5m27s  (67.8%)39.6%  lr: 0.015826  loss: 0.007688  eta: 0h2m  tot: 0h5m28s  (67.9%)40.1%  lr: 0.015756  loss: 0.007717  eta: 0h2m  tot: 0h5m29s  (68.0%)42.5%  lr: 0.015496  loss: 0.007725  eta: 0h2m  tot: 0h5m31s  (68.5%)42.9%  lr: 0.015425  loss: 0.007720  eta: 0h2m  tot: 0h5m31s  (68.6%)44.9%  lr: 0.015305  loss: 0.007772  eta: 0h2m  tot: 0h5m33s  (69.0%)45.7%  lr: 0.015235  loss: 0.007760  eta: 0h2m  tot: 0h5m34s  (69.1%)50.1%  lr: 0.014755  loss: 0.007852  eta: 0h2m  tot: 0h5m38s  (70.0%)50.4%  lr: 0.014735  loss: 0.007860  eta: 0h2m  tot: 0h5m38s  (70.1%)0h5m39s  (70.1%)50.8%  lr: 0.014725  loss: 0.007874  eta: 0h2m  tot: 0h5m39s  (70.2%)51.6%  lr: 0.014675  loss: 0.007864  eta: 0h2m  tot: 0h5m39s  (70.3%)51.7%  lr: 0.014665  loss: 0.007859  eta: 0h2m  tot: 0h5m40s  (70.3%)51.9%  lr: 0.014625  loss: 0.007862  eta: 0h2m  tot: 0h5m40s  (70.4%)52.4%  lr: 0.014595  loss: 0.007852  eta: 0h2m  tot: 0h5m40s  (70.5%)52.8%  lr: 0.014565  loss: 0.007844  eta: 0h2m  tot: 0h5m41s  (70.6%)53.9%  lr: 0.014445  loss: 0.007835  eta: 0h2m  tot: 0h5m42s  (70.8%)54.6%  lr: 0.014314  loss: 0.007848  eta: 0h2m  tot: 0h5m42s  (70.9%)55.2%  lr: 0.014274  loss: 0.007847  eta: 0h2m  tot: 0h5m43s  (71.0%)55.7%  lr: 0.014234  loss: 0.007854  eta: 0h2m  tot: 0h5m44s  (71.1%)57.3%  lr: 0.014114  loss: 0.007809  eta: 0h2m  tot: 0h5m45s  (71.5%)58.0%  lr: 0.013984  loss: 0.007804  eta: 0h2m  tot: 0h5m46s  (71.6%)60.5%  lr: 0.013784  loss: 0.007758  eta: 0h2m  tot: 0h5m48s  (72.1%)61.0%  lr: 0.013754  loss: 0.007760  eta: 0h2m  tot: 0h5m48s  (72.2%)61.5%  lr: 0.013684  loss: 0.007772  eta: 0h2m  tot: 0h5m49s  (72.3%)%  lr: 0.013674  loss: 0.007769  eta: 0h2m  tot: 0h5m50s  (72.5%)64.3%  lr: 0.013474  loss: 0.007739  eta: 0h2m  tot: 0h5m51s  (72.9%)64.6%  lr: 0.013444  loss: 0.007748  eta: 0h2m  tot: 0h5m52s  (72.9%)66.3%  lr: 0.013313  loss: 0.007787  eta: 0h2m  tot: 0h5m53s  (73.3%)67.3%  lr: 0.013183  loss: 0.007779  eta: 0h2m  tot: 0h5m54s  (73.5%)67.6%  lr: 0.013163  loss: 0.007771  eta: 0h2m  tot: 0h5m54s  (73.5%)67.7%  lr: 0.013163  loss: 0.007779  eta: 0h2m  tot: 0h5m55s  (73.5%)68.2%  lr: 0.013123  loss: 0.007771  eta: 0h2m  tot: 0h5m55s  (73.6%)69.8%  lr: 0.012923  loss: 0.007792  eta: 0h2m  tot: 0h5m57s  (74.0%)70.3%  lr: 0.012853  loss: 0.007801  eta: 0h2m  tot: 0h5m57s  (74.1%)71.0%  lr: 0.012793  loss: 0.007818  eta: 0h2m  tot: 0h5m58s  (74.2%)72.4%  lr: 0.012633  loss: 0.007834  eta: 0h1m  tot: 0h5m59s  (74.5%)72.9%  lr: 0.012563  loss: 0.007823  eta: 0h1m  tot: 0h5m59s  (74.6%)73.0%  lr: 0.012553  loss: 0.007820  eta: 0h1m  tot: 0h6m0s  (74.6%)73.5%  lr: 0.012533  loss: 0.007828  eta: 0h1m  tot: 0h6m0s  (74.7%)74.4%  lr: 0.012393  loss: 0.007817  eta: 0h1m  tot: 0h6m1s  (74.9%)74.5%  lr: 0.012382  loss: 0.007818  eta: 0h1m  tot: 0h6m1s  (74.9%)75.4%  lr: 0.012282  loss: 0.007814  eta: 0h1m  tot: 0h6m2s  (75.1%)78.0%  lr: 0.011912  loss: 0.007825  eta: 0h1m  tot: 0h6m4s  (75.6%)78.6%  lr: 0.011892  loss: 0.007834  eta: 0h1m  tot: 0h6m5s  (75.7%)79.4%  lr: 0.011792  loss: 0.007837  eta: 0h1m  tot: 0h6m5s  (75.9%)81.1%  lr: 0.011672  loss: 0.007836  eta: 0h1m  tot: 0h6m7s  (76.2%)83.1%  lr: 0.011422  loss: 0.007816  eta: 0h1m  tot: 0h6m9s  (76.6%)83.7%  lr: 0.011372  loss: 0.007802  eta: 0h1m  tot: 0h6m9s  (76.7%)84.1%  lr: 0.011361  loss: 0.007792  eta: 0h1m  tot: 0h6m10s  (76.8%)84.8%  lr: 0.011291  loss: 0.007789  eta: 0h1m  tot: 0h6m10s  (77.0%)%  lr: 0.011291  loss: 0.007785  eta: 0h1m  tot: 0h6m10s  (77.0%)85.0%  lr: 0.011271  loss: 0.007780  eta: 0h1m  tot: 0h6m11s  (77.0%)87.1%  lr: 0.011081  loss: 0.007766  eta: 0h1m  tot: 0h6m12s  (77.4%)87.2%  lr: 0.011081  loss: 0.007771  eta: 0h1m  tot: 0h6m13s  (77.4%)88.5%  lr: 0.010941  loss: 0.007752  eta: 0h1m  tot: 0h6m14s  (77.7%)88.8%  lr: 0.010931  loss: 0.007756  eta: 0h1m  tot: 0h6m14s  (77.8%)88.9%  lr: 0.010911  loss: 0.007762  eta: 0h1m  tot: 0h6m14s  (77.8%)89.8%  lr: 0.010831  loss: 0.007787  eta: 0h1m  tot: 0h6m15s  (78.0%)90.4%  lr: 0.010811  loss: 0.007779  eta: 0h1m  tot: 0h6m15s  (78.1%)90.5%  lr: 0.010811  loss: 0.007783  eta: 0h1m  tot: 0h6m16s  (78.1%)90.7%  lr: 0.010811  loss: 0.007783  eta: 0h1m  tot: 0h6m16s  (78.1%)90.9%  lr: 0.010791  loss: 0.007781  eta: 0h1m  tot: 0h6m16s  (78.2%)91.3%  lr: 0.010771  loss: 0.007775  eta: 0h1m  tot: 0h6m16s  (78.3%)92.1%  lr: 0.010651  loss: 0.007773  eta: 0h1m  tot: 0h6m17s  (78.4%)%  lr: 0.010461  loss: 0.007770  eta: 0h1m  tot: 0h6m19s  (78.8%)94.3%  lr: 0.010431  loss: 0.007769  eta: 0h1m  tot: 0h6m19s  (78.9%)94.8%  lr: 0.010391  loss: 0.007770  eta: 0h1m  tot: 0h6m20s  (79.0%)96.1%  lr: 0.010230  loss: 0.007776  eta: 0h1m  tot: 0h6m21s  (79.2%)96.5%  lr: 0.010180  loss: 0.007774  eta: 0h1m  tot: 0h6m22s  (79.3%)%  lr: 0.010040  loss: 0.007784  eta: 0h1m  tot: 0h6m22s  (79.5%)\n",
      " ---+++                Epoch    3 Train error : 0.00772887 +++--- ☃\n",
      "Training epoch 4: 0.01 0.01\n",
      "Epoch: 100.0%  lr: 0.000000  loss: 0.006837  eta: <1min   tot: 0h7m56s  (100.0%)%  lr: 0.009880  loss: 0.006124  eta: 0h1m  tot: 0h6m25s  (80.4%)2.2%  lr: 0.009880  loss: 0.006183  eta: 0h1m  tot: 0h6m26s  (80.4%)3.0%  lr: 0.009760  loss: 0.005850  eta: 0h1m  tot: 0h6m26s  (80.6%)m  tot: 0h6m27s  (80.6%)5.3%  lr: 0.009550  loss: 0.006466  eta: 0h1m  tot: 0h6m28s  (81.1%)5.9%  lr: 0.009489  loss: 0.006360  eta: 0h1m  tot: 0h6m29s  (81.2%)8.1%  lr: 0.009209  loss: 0.006779  eta: 0h1m  tot: 0h6m31s  (81.6%)10.5%  lr: 0.008989  loss: 0.006695  eta: 0h1m  tot: 0h6m33s  (82.1%)10.6%  lr: 0.008969  loss: 0.006710  eta: 0h1m  tot: 0h6m34s  (82.1%)14.0%  lr: 0.008629  loss: 0.006806  eta: 0h1m  tot: 0h6m37s  (82.8%)14.3%  lr: 0.008609  loss: 0.006815  eta: 0h1m  tot: 0h6m37s  (82.9%)15.3%  lr: 0.008539  loss: 0.006777  eta: 0h1m  tot: 0h6m38s  (83.1%)15.5%  lr: 0.008529  loss: 0.006795  eta: 0h1m  tot: 0h6m38s  (83.1%)16.3%  lr: 0.008468  loss: 0.006931  eta: 0h1m  tot: 0h6m39s  (83.3%)16.5%  lr: 0.008448  loss: 0.006905  eta: 0h1m  tot: 0h6m39s  (83.3%)16.7%  lr: 0.008448  loss: 0.006959  eta: 0h1m  tot: 0h6m39s  (83.3%)16.8%  lr: 0.008428  loss: 0.006970  eta: 0h1m  tot: 0h6m39s  (83.4%)17.4%  lr: 0.008368  loss: 0.006990  eta: 0h1m  tot: 0h6m40s  (83.5%)17.8%  lr: 0.008318  loss: 0.006975  eta: 0h1m  tot: 0h6m40s  (83.6%)18.5%  lr: 0.008238  loss: 0.007063  eta: 0h1m  tot: 0h6m41s  (83.7%)18.9%  lr: 0.008228  loss: 0.007017  eta: 0h1m  tot: 0h6m41s  (83.8%)19.2%  lr: 0.008228  loss: 0.006994  eta: 0h1m  tot: 0h6m41s  (83.8%)21.6%  lr: 0.007998  loss: 0.007102  eta: 0h1m  tot: 0h6m44s  (84.3%)21.9%  lr: 0.007948  loss: 0.007075  eta: 0h1m  tot: 0h6m44s  (84.4%)22.9%  lr: 0.007888  loss: 0.007019  eta: 0h1m  tot: 0h6m45s  (84.6%)24.9%  lr: 0.007698  loss: 0.007021  eta: 0h1m  tot: 0h6m47s  (85.0%)25.6%  lr: 0.007618  loss: 0.006962  eta: 0h1m  tot: 0h6m47s  (85.1%)26.0%  lr: 0.007588  loss: 0.006990  eta: 0h1m  tot: 0h6m48s  (85.2%)27.8%  lr: 0.007437  loss: 0.007024  eta: 0h1m  tot: 0h6m49s  (85.6%)28.2%  lr: 0.007397  loss: 0.007047  eta: 0h1m  tot: 0h6m50s  (85.6%)29.7%  lr: 0.007267  loss: 0.006965  eta: 0h1m  tot: 0h6m51s  (85.9%)30.3%  lr: 0.007197  loss: 0.006995  eta: 0h1m  tot: 0h6m52s  (86.1%)30.5%  lr: 0.007177  loss: 0.006994  eta: 0h1m  tot: 0h6m52s  (86.1%)33.7%  lr: 0.006897  loss: 0.007043  eta: 0h1m  tot: 0h6m55s  (86.7%)34.7%  lr: 0.006787  loss: 0.007028  eta: 0h1m  tot: 0h6m56s  (86.9%)35.0%  lr: 0.006747  loss: 0.007034  eta: 0h1m  tot: 0h6m56s  (87.0%)0.007031  eta: 0h1m  tot: 0h6m56s  (87.0%)35.4%  lr: 0.006687  loss: 0.007043  eta: <1min   tot: 0h6m56s  (87.1%)s  (87.1%)35.9%  lr: 0.006637  loss: 0.007028  eta: <1min   tot: 0h6m57s  (87.2%)38.2%  lr: 0.006296  loss: 0.007016  eta: <1min   tot: 0h6m59s  (87.6%)38.5%  lr: 0.006246  loss: 0.006987  eta: <1min   tot: 0h6m59s  (87.7%)39.1%  lr: 0.006216  loss: 0.006978  eta: <1min   tot: 0h7m0s  (87.8%)39.2%  lr: 0.006206  loss: 0.006977  eta: <1min   tot: 0h7m0s  (87.8%)39.3%  lr: 0.006196  loss: 0.007009  eta: <1min   tot: 0h7m0s  (87.9%)39.5%  lr: 0.006186  loss: 0.007054  eta: <1min   tot: 0h7m0s  (87.9%)39.6%  lr: 0.006176  loss: 0.007043  eta: <1min   tot: 0h7m0s  (87.9%)40.0%  lr: 0.006136  loss: 0.007049  eta: <1min   tot: 0h7m0s  (88.0%)40.1%  lr: 0.006126  loss: 0.007038  eta: <1min   tot: 0h7m1s  (88.0%)40.3%  lr: 0.006096  loss: 0.007051  eta: <1min   tot: 0h7m1s  (88.1%)41.0%  lr: 0.005976  loss: 0.007066  eta: <1min   tot: 0h7m1s  (88.2%)0.007050  eta: <1min   tot: 0h7m2s  (88.2%)41.5%  lr: 0.005926  loss: 0.007051  eta: <1min   tot: 0h7m2s  (88.3%)42.5%  lr: 0.005806  loss: 0.007020  eta: <1min   tot: 0h7m3s  (88.5%)42.7%  lr: 0.005766  loss: 0.007015  eta: <1min   tot: 0h7m3s  (88.5%)42.9%  lr: 0.005736  loss: 0.007038  eta: <1min   tot: 0h7m3s  (88.6%)45.4%  lr: 0.005415  loss: 0.007011  eta: <1min   tot: 0h7m6s  (89.1%)46.9%  lr: 0.005275  loss: 0.006951  eta: <1min   tot: 0h7m7s  (89.4%)47.6%  lr: 0.005175  loss: 0.006978  eta: <1min   tot: 0h7m8s  (89.5%)48.7%  lr: 0.005055  loss: 0.006948  eta: <1min   tot: 0h7m9s  (89.7%)48.8%  lr: 0.005035  loss: 0.006942  eta: <1min   tot: 0h7m9s  (89.8%)49.0%  lr: 0.005005  loss: 0.006935  eta: <1min   tot: 0h7m9s  (89.8%)50.1%  lr: 0.004925  loss: 0.006945  eta: <1min   tot: 0h7m10s  (90.0%)50.4%  lr: 0.004865  loss: 0.006946  eta: <1min   tot: 0h7m10s  (90.1%)51.9%  lr: 0.004665  loss: 0.006975  eta: <1min   tot: 0h7m12s  (90.4%)53.0%  lr: 0.004575  loss: 0.007006  eta: <1min   tot: 0h7m13s  (90.6%)54.1%  lr: 0.004475  loss: 0.006988  eta: <1min   tot: 0h7m14s  (90.8%)55.5%  lr: 0.004314  loss: 0.007003  eta: <1min   tot: 0h7m15s  (91.1%)56.7%  lr: 0.004224  loss: 0.006982  eta: <1min   tot: 0h7m16s  (91.3%)91.4%)57.0%  lr: 0.004194  loss: 0.006972  eta: <1min   tot: 0h7m17s  (91.4%)60.2%  lr: 0.003944  loss: 0.007014  eta: <1min   tot: 0h7m20s  (92.0%)62.1%  lr: 0.003724  loss: 0.007004  eta: <1min   tot: 0h7m21s  (92.4%)62.4%  lr: 0.003704  loss: 0.007005  eta: <1min   tot: 0h7m22s  (92.5%)62.7%  lr: 0.003684  loss: 0.007000  eta: <1min   tot: 0h7m22s  (92.5%)63.4%  lr: 0.003604  loss: 0.007013  eta: <1min   tot: 0h7m22s  (92.7%)63.6%  lr: 0.003574  loss: 0.007012  eta: <1min   tot: 0h7m23s  (92.7%)64.2%  lr: 0.003484  loss: 0.007022  eta: <1min   tot: 0h7m23s  (92.8%)64.7%  lr: 0.003434  loss: 0.007011  eta: <1min   tot: 0h7m24s  (92.9%)66.5%  lr: 0.003283  loss: 0.006991  eta: <1min   tot: 0h7m25s  (93.3%)67.2%  lr: 0.003223  loss: 0.006982  eta: <1min   tot: 0h7m26s  (93.4%)67.6%  lr: 0.003183  loss: 0.006974  eta: <1min   tot: 0h7m26s  (93.5%)67.9%  lr: 0.003143  loss: 0.006964  eta: <1min   tot: 0h7m27s  (93.6%)68.4%  lr: 0.003093  loss: 0.006971  eta: <1min   tot: 0h7m27s  (93.7%)68.6%  lr: 0.003083  loss: 0.006971  eta: <1min   tot: 0h7m27s  (93.7%)69.2%  lr: 0.003053  loss: 0.006990  eta: <1min   tot: 0h7m28s  (93.8%)69.4%  lr: 0.003013  loss: 0.006982  eta: <1min   tot: 0h7m28s  (93.9%)69.5%  lr: 0.002993  loss: 0.006981  eta: <1min   tot: 0h7m28s  (93.9%)71.7%  lr: 0.002783  loss: 0.006976  eta: <1min   tot: 0h7m30s  (94.3%)72.6%  lr: 0.002683  loss: 0.006961  eta: <1min   tot: 0h7m31s  (94.5%)72.6%  lr: 0.002683  loss: 0.006954  eta: <1min   tot: 0h7m31s  (94.5%)74.0%  lr: 0.002493  loss: 0.006972  eta: <1min   tot: 0h7m33s  (94.8%)h7m33s  (94.8%)76.8%  lr: 0.002162  loss: 0.006935  eta: <1min   tot: 0h7m35s  (95.4%)78.0%  lr: 0.002062  loss: 0.006931  eta: <1min   tot: 0h7m36s  (95.6%)79.1%  lr: 0.001902  loss: 0.006909  eta: <1min   tot: 0h7m37s  (95.8%)81.0%  lr: 0.001752  loss: 0.006910  eta: <1min   tot: 0h7m39s  (96.2%)81.4%  lr: 0.001702  loss: 0.006911  eta: <1min   tot: 0h7m39s  (96.3%)82.1%  lr: 0.001612  loss: 0.006905  eta: <1min   tot: 0h7m40s  (96.4%)  eta: <1min   tot: 0h7m40s  (96.5%)h7m41s  (96.6%)83.3%  lr: 0.001442  loss: 0.006896  eta: <1min   tot: 0h7m42s  (96.7%)84.5%  lr: 0.001401  loss: 0.006905  eta: <1min   tot: 0h7m43s  (96.9%)85.1%  lr: 0.001311  loss: 0.006902  eta: <1min   tot: 0h7m43s  (97.0%) (97.2%)86.6%  lr: 0.001091  loss: 0.006876  eta: <1min   tot: 0h7m45s  (97.3%)87.1%  lr: 0.001011  loss: 0.006873  eta: <1min   tot: 0h7m45s  (97.4%)88.0%  lr: 0.000941  loss: 0.006867  eta: <1min   tot: 0h7m46s  (97.6%)89.4%  lr: 0.000831  loss: 0.006875  eta: <1min   tot: 0h7m48s  (97.9%)91.1%  lr: 0.000721  loss: 0.006873  eta: <1min   tot: 0h7m49s  (98.2%)91.5%  lr: 0.000711  loss: 0.006864  eta: <1min   tot: 0h7m50s  (98.3%)91.8%  lr: 0.000681  loss: 0.006866  eta: <1min   tot: 0h7m50s  (98.4%)91.9%  lr: 0.000671  loss: 0.006871  eta: <1min   tot: 0h7m50s  (98.4%)92.6%  lr: 0.000611  loss: 0.006857  eta: <1min   tot: 0h7m51s  (98.5%)92.7%  lr: 0.000591  loss: 0.006865  eta: <1min   tot: 0h7m51s  (98.5%)93.5%  lr: 0.000511  loss: 0.006863  eta: <1min   tot: 0h7m52s  (98.7%)  loss: 0.006862  eta: <1min   tot: 0h7m53s  (98.9%)95.5%  lr: 0.000200  loss: 0.006851  eta: <1min   tot: 0h7m54s  (99.1%)96.0%  lr: 0.000190  loss: 0.006859  eta: <1min   tot: 0h7m54s  (99.2%)96.6%  lr: 0.000150  loss: 0.006856  eta: <1min   tot: 0h7m55s  (99.3%)\n",
      " ---+++                Epoch    4 Train error : 0.00687793 +++--- ☃\n",
      "Saving model to file : data/my_model\n",
      "Saving model in tsv format : data/my_model.tsv\n"
     ]
    }
   ],
   "source": [
    "######### TRAINING HAPPENING HERE #############\n",
    "!/home/ubuntu/code/StarSpace/starspace train --trainFile 'data/train_prepared.tsv' \\\n",
    "    -model 'data/my_model' -trainMode 3 -adagrad true -ngrams 1 \\\n",
    "    -epoch 5 -dim 100 -minCount 2 -verbose true -fileFormat 'labelDoc' \\\n",
    "    -negSearchLimit 10 -similarity \"cosine\" -lr 0.05"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "And now we can compare the new embeddings with the previous ones. You can find trained word vectors in the file *[model_file_name].tsv*. Upload the embeddings from StarSpace into a dict. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [],
   "source": [
    "starspace_embeddings = {}\n",
    "for line in open('data/my_model.tsv'):\n",
    "    word,*vec = line.strip().split()\n",
    "    vf = [float(v) for v in vec]\n",
    "    starspace_embeddings[word] = np.array(vf)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [],
   "source": [
    "ss_prepared_ranking = []\n",
    "for line in prepared_validation:\n",
    "    q, *ex = line\n",
    "    ranks = rank_candidates(q, ex, starspace_embeddings, 100)\n",
    "    ss_prepared_ranking.append([r[0] for r in ranks].index(0) + 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DCG@   1: 0.516 | Hits@   1: 0.516\n",
      "DCG@   5: 0.614 | Hits@   5: 0.697\n",
      "DCG@  10: 0.632 | Hits@  10: 0.753\n",
      "DCG@ 100: 0.662 | Hits@ 100: 0.899\n",
      "DCG@ 500: 0.673 | Hits@ 500: 0.980\n",
      "DCG@1000: 0.675 | Hits@1000: 1.000\n"
     ]
    }
   ],
   "source": [
    "for k in [1, 5, 10, 100, 500, 1000]:\n",
    "    print(\"DCG@%4d: %.3f | Hits@%4d: %.3f\" % (k, dcg_score(ss_prepared_ranking, k), \n",
    "                                               k, hits_count(ss_prepared_ranking, k)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Due to training for the particular task with the supervised data, you should expect to obtain a higher quality than for the previous approach. In additiion, despite the fact that StarSpace's trained vectors have a smaller dimension than word2vec's, it provides better results in this task."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Task 5 (StarSpaceRanks).** For each question from prepared *test.tsv* submit the ranks of the candidates for trained representation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "starspace_ranks_results = []\n",
    "prepared_test_data = ######### YOUR CODE HERE #############\n",
    "for line in open(prepared_test_data):\n",
    "    q, *ex = line.strip().split('\\t')\n",
    "    ranks = rank_candidates(q, ex, starspace_embeddings, 100)\n",
    "    ranked_candidates = [r[0] for r in ranks]\n",
    "    starspace_ranks_results.append([ranked_candidates.index(i) + 1 for i in range(len(ranked_candidates))])\n",
    "    \n",
    "grader.submit_tag('StarSpaceRanks', matrix_to_string(starspace_ranks_results))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Please, **don't remove** the file with these embeddings because you will need them in the final project."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Authorization & Submission\n",
    "To submit assignment parts to Cousera platform, please, enter your e-mail and token into variables below. You can generate token on this programming assignment page. <b>Note:</b> Token expires 30 minutes after generation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "STUDENT_EMAIL = # EMAIL \n",
    "STUDENT_TOKEN = # TOKEN \n",
    "grader.status()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you want to submit these answers, run cell below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "grader.submit(STUDENT_EMAIL, STUDENT_TOKEN)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Environment (conda_python_dl)",
   "language": "python",
   "name": "conda_python_dl"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
